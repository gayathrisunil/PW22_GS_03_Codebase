{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO0efz7FKcNH",
        "outputId": "01321578-6ed3-48b4-f600-dfa7bff956a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34Sj6CEbxsHH",
        "outputId": "4aeec8b2-7296-4642-c720-6d52d4612b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stellargraph\n",
            "  Downloading stellargraph-1.2.1-py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.3.5)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.9.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.0.2)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (5.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->stellargraph) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->stellargraph) (2022.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.19.6)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (14.0.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.50.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (21.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.27.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.1.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.12)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.9.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->stellargraph) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.1.0->stellargraph) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (2.14.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (1.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->stellargraph) (3.2.2)\n",
            "Installing collected packages: stellargraph\n",
            "Successfully installed stellargraph-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "#installs\n",
        "\n",
        "!pip install stellargraph\n",
        "!pip install pyyaml h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrtnMu3bNDm4"
      },
      "outputs": [],
      "source": [
        "#imports \n",
        "\n",
        "from stellargraph import StellarGraph\n",
        "import stellargraph as sg\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import GCNSupervisedGraphClassification, DeepGraphCNN\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import model_selection\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import f1_score, precision_score, confusion_matrix, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3bnXeuXM_am"
      },
      "source": [
        "Creating custom graph dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-Zv-fioPfd0"
      },
      "outputs": [],
      "source": [
        "combined_phenotype='/content/drive/MyDrive/Capstone/COMBINED DATA/phenotypic_data.csv'\n",
        "aug2_label = '/content/drive/MyDrive/Capstone/COMBINED DATA/aug2_labels.csv'\n",
        "labels_mappings= pd.read_csv(combined_phenotype)\n",
        "aug2_labels = pd.read_csv(aug2_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DazwG5AQoUn"
      },
      "outputs": [],
      "source": [
        "def row_transform(arr, threshold):\n",
        "  for i in range(len(arr)):\n",
        "      arr[i] = arr[i] if arr[i]> threshold else abs(arr[i])+0.0001\n",
        "  return arr\n",
        "\n",
        "def binarize(df, threshold):\n",
        "    df = df.transform(lambda x: row_transform(x, threshold))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKnL6r1UxYBe"
      },
      "outputs": [],
      "source": [
        "cb = '/content/drive/MyDrive/Capstone/COMBINED DATA/COBRE Binary'\n",
        "cw = '/content/drive/MyDrive/Capstone/COMBINED DATA/COBRE Weighted'\n",
        "\n",
        "ub = '/content/drive/MyDrive/Capstone/COMBINED DATA/UCLA Binary v2'\n",
        "uw = '/content/drive/MyDrive/Capstone/COMBINED DATA/UCLA Weighted v2'\n",
        "\n",
        "uba = '/content/drive/MyDrive/Capstone/Data Augmentation/UCLA Augmented Binary/UCLA Binary 0_2'\n",
        "uwa = '/content/drive/MyDrive/Capstone/Data Augmentation/UCLA Augmented'\n",
        "\n",
        "uba1 = '/content/drive/MyDrive/Capstone/Data Augmentation/UCLA Augmented Method 2 Binary/UCLA Binary 0_2'\n",
        "uwa1 = '/content/drive/MyDrive/Capstone/Data Augmentation/UCLA Augmented Method 2'\n",
        "\n",
        "weighted_features_aug = '/content/drive/MyDrive/Capstone/Data Augmentation/Local Weighted features - UCLA Aug/'\n",
        "binary_features_aug = '/content/drive/MyDrive/Capstone/Data Augmentation/Local Binary features -  UCLA Aug/'\n",
        "\n",
        "weighted_features_aug1 = '/content/drive/MyDrive/Capstone/Data Augmentation/Local Weighted features- UCLA Aug v2/'\n",
        "binary_features_aug1 = '/content/drive/MyDrive/Capstone/Data Augmentation/Local Binary features - UCLA Aug v2/'\n",
        "\n",
        "binary_features_02 = '/content/drive/MyDrive/Capstone/Graph properties /UCLA Binary Local 0_2/'\n",
        "weighted_features = '/content/drive/MyDrive/Capstone/Graph properties /Weighted local feature files/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT7b-jlwqJ4W"
      },
      "outputs": [],
      "source": [
        "#0.25 threshold\n",
        "ub_02 = '/content/drive/MyDrive/Capstone/COMBINED DATA/UCLA Binary 0_2/'\n",
        "binary_features = '/content/drive/MyDrive/Capstone/Graph properties /UCLA Binary Local 0_2/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SHQCSrjwezy"
      },
      "outputs": [],
      "source": [
        "graphs_dir = [uba1, uba, ub_02]\n",
        "# graphs_dir = [ub]\n",
        "feat_dir = [(binary_features, \"binary_node_features_ucla_binary_\"), (weighted_features, \"weighted_node_features_\")]\n",
        "aug_feat_dir = [(binary_features_aug, \"binary_node_features_\"), (weighted_features_aug, \"weighted_node_features_\")]\n",
        "aug_feat_dir1 = [(binary_features_aug1, \"binary_node_features_\"), (weighted_features_aug1, \"weighted_node_features_\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikl0M337zW3q"
      },
      "outputs": [],
      "source": [
        "in_feat=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80-tILrruzCv",
        "outputId": "f4495034-02af-4ff6-dd12-c594bb2c6f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4018\n",
            "4021\n",
            "4023\n",
            "4014\n",
            "4016\n",
            "4019\n",
            "4024\n",
            "4022\n",
            "4025\n",
            "4009\n",
            "4017\n",
            "4000\n",
            "4001\n",
            "4006\n",
            "4015\n",
            "4005\n",
            "4002\n",
            "4003\n",
            "4013\n",
            "4008\n",
            "4010\n",
            "4007\n",
            "4020\n",
            "4004\n",
            "4011\n",
            "4012\n",
            "4041\n",
            "4043\n",
            "4061\n",
            "4066\n",
            "4034\n",
            "4032\n",
            "4051\n",
            "4069\n",
            "4067\n",
            "4029\n",
            "4035\n",
            "4037\n",
            "4028\n",
            "4039\n",
            "4062\n",
            "4060\n",
            "4038\n",
            "4058\n",
            "4047\n",
            "4055\n",
            "4072\n",
            "4075\n",
            "4050\n",
            "4048\n",
            "4074\n",
            "4042\n",
            "4068\n",
            "4026\n",
            "4073\n",
            "4057\n",
            "4054\n",
            "4064\n",
            "4052\n",
            "4033\n",
            "4030\n",
            "4045\n",
            "4071\n",
            "4040\n",
            "4056\n",
            "4065\n",
            "4063\n",
            "4053\n",
            "4031\n",
            "4070\n",
            "4036\n",
            "4059\n",
            "4027\n",
            "4049\n",
            "4044\n",
            "4046\n",
            "4098\n",
            "4113\n",
            "4103\n",
            "4107\n",
            "4077\n",
            "4120\n",
            "4100\n",
            "4109\n",
            "4080\n",
            "4086\n",
            "4091\n",
            "4078\n",
            "4079\n",
            "4093\n",
            "4112\n",
            "4111\n",
            "4090\n",
            "4089\n",
            "4104\n",
            "4101\n",
            "4108\n",
            "4087\n",
            "4083\n",
            "4097\n",
            "4123\n",
            "4118\n",
            "4117\n",
            "4114\n",
            "4124\n",
            "4085\n",
            "4099\n",
            "4102\n",
            "4116\n",
            "4084\n",
            "4119\n",
            "4096\n",
            "4088\n",
            "4106\n",
            "4105\n",
            "4121\n",
            "4094\n",
            "4125\n",
            "4092\n",
            "4082\n",
            "4110\n",
            "4081\n",
            "4115\n",
            "4122\n",
            "4095\n",
            "4147\n",
            "4076\n",
            "4169\n",
            "4129\n",
            "4128\n",
            "4144\n",
            "4150\n",
            "4141\n",
            "4142\n",
            "4154\n",
            "4165\n",
            "4143\n",
            "4146\n",
            "4151\n",
            "4137\n",
            "4161\n",
            "4156\n",
            "4157\n",
            "4133\n",
            "4135\n",
            "4167\n",
            "4136\n",
            "4168\n",
            "4164\n",
            "4127\n",
            "4170\n",
            "4148\n",
            "4171\n",
            "4166\n",
            "4153\n",
            "4158\n",
            "4159\n",
            "4139\n",
            "4162\n",
            "4145\n",
            "4163\n",
            "4149\n",
            "4140\n",
            "4130\n",
            "4134\n",
            "4126\n",
            "4131\n",
            "4160\n",
            "4155\n",
            "4138\n",
            "4152\n",
            "4132\n",
            "3125\n",
            "3126\n",
            "3129\n",
            "3128\n",
            "3130\n",
            "3133\n",
            "3131\n",
            "3135\n",
            "3132\n",
            "3134\n",
            "3136\n",
            "3138\n",
            "3137\n",
            "3139\n",
            "3140\n",
            "3141\n",
            "3142\n",
            "3144\n",
            "3143\n",
            "3146\n",
            "3145\n",
            "3148\n",
            "3149\n",
            "3147\n",
            "3151\n",
            "3152\n",
            "3154\n",
            "3155\n",
            "3153\n",
            "3158\n",
            "3156\n",
            "3161\n",
            "3159\n",
            "3162\n",
            "3157\n",
            "3163\n",
            "3164\n",
            "3165\n",
            "3168\n",
            "3167\n",
            "3170\n",
            "3169\n",
            "3171\n",
            "3166\n",
            "3160\n",
            "3150\n",
            "2019\n",
            "2024\n",
            "2022\n",
            "2015\n",
            "2017\n",
            "2020\n",
            "2025\n",
            "2026\n",
            "2023\n",
            "2010\n",
            "2018\n",
            "2001\n",
            "2002\n",
            "2007\n",
            "2016\n",
            "2006\n",
            "2014\n",
            "2003\n",
            "2004\n",
            "2009\n",
            "2008\n",
            "2011\n",
            "2005\n",
            "2012\n",
            "2021\n",
            "2013\n",
            "2042\n",
            "2044\n",
            "2067\n",
            "2062\n",
            "2035\n",
            "2070\n",
            "2033\n",
            "2052\n",
            "2030\n",
            "2068\n",
            "2029\n",
            "2038\n",
            "2036\n",
            "2040\n",
            "2063\n",
            "2061\n",
            "2059\n",
            "2056\n",
            "2039\n",
            "2048\n",
            "2073\n",
            "2076\n",
            "2051\n",
            "2075\n",
            "2043\n",
            "2069\n",
            "2049\n",
            "2027\n",
            "2074\n",
            "2058\n",
            "2055\n",
            "2065\n",
            "2034\n",
            "2046\n",
            "2053\n",
            "2031\n",
            "2072\n",
            "2041\n",
            "2057\n",
            "2066\n",
            "2064\n",
            "2032\n",
            "2054\n",
            "2071\n",
            "2037\n",
            "2028\n",
            "2060\n",
            "2045\n",
            "2047\n",
            "2050\n",
            "2099\n",
            "2114\n",
            "2108\n",
            "2104\n",
            "2101\n",
            "2121\n",
            "2078\n",
            "2110\n",
            "2081\n",
            "2087\n",
            "2092\n",
            "2079\n",
            "2113\n",
            "2080\n",
            "2094\n",
            "2112\n",
            "2091\n",
            "2090\n",
            "2105\n",
            "2102\n",
            "2088\n",
            "2109\n",
            "2124\n",
            "2098\n",
            "2084\n",
            "2118\n",
            "2119\n",
            "2115\n",
            "2125\n",
            "2086\n",
            "2100\n",
            "2117\n",
            "2103\n",
            "2085\n",
            "2097\n",
            "2120\n",
            "2107\n",
            "2089\n",
            "2106\n",
            "2095\n",
            "2126\n",
            "2122\n",
            "2093\n",
            "2083\n",
            "2111\n",
            "2082\n",
            "2123\n",
            "2116\n",
            "2096\n",
            "2077\n",
            "2148\n",
            "2130\n",
            "2171\n",
            "2129\n",
            "2151\n",
            "2145\n",
            "2142\n",
            "2156\n",
            "2143\n",
            "2167\n",
            "2144\n",
            "2147\n",
            "2152\n",
            "2163\n",
            "2138\n",
            "2159\n",
            "2158\n",
            "2134\n",
            "2169\n",
            "2136\n",
            "2137\n",
            "2166\n",
            "2128\n",
            "2170\n",
            "2149\n",
            "2168\n",
            "2160\n",
            "2172\n",
            "2154\n",
            "2161\n",
            "2140\n",
            "2164\n",
            "2165\n",
            "2150\n",
            "2146\n",
            "2141\n",
            "2131\n",
            "2135\n",
            "2127\n",
            "2132\n",
            "2157\n",
            "2162\n",
            "2139\n",
            "2133\n",
            "2153\n"
          ]
        }
      ],
      "source": [
        "graphs = list()\n",
        "graph_labels = list()\n",
        "\n",
        "# Graph creation\n",
        "for directory in graphs_dir:\n",
        "  for file in os.listdir(directory):\n",
        "    if '(' in file:\n",
        "        continue\n",
        "    if file.endswith('.csv'):\n",
        "      if (directory == cb):\n",
        "        subject = file[13:18]\n",
        "      elif (directory == cw):\n",
        "          subject = file[15:20]\n",
        "      elif (directory == ub_02):\n",
        "          subject = file[12:16]\n",
        "          if int(subject) == 2155:\n",
        "            continue\n",
        "      elif (directory == uw):\n",
        "          subject = file[14:18]\n",
        "      elif (directory == uba):\n",
        "          subject = file[12:16]\n",
        "          if int(subject) < 3125:\n",
        "            continue\n",
        "      elif (directory == uba1):\n",
        "          subject = file[12:16]\n",
        "      else:\n",
        "        print(\"INVALID DIRECTORY\")\n",
        "        exit(0)\n",
        "\n",
        "\n",
        "      # APPENDING LABEL\n",
        "      subject = int(subject)\n",
        "      print(subject)\n",
        "      mask = labels_mappings['Subject'] == subject\n",
        "      #only for aug v2\n",
        "      if directory ==  uba1:\n",
        "        if(labels_mappings[mask]['Label'].values[0]==0 or \n",
        "            (labels_mappings[mask]['Label'].values[0]!=aug2_labels.query(\"Subject==@subject\")[\"assumed\"].values[0])):\n",
        "            continue\n",
        "      graph_labels.append(labels_mappings[mask]['Label'].values[0])\n",
        "\n",
        "      # APPENDING CORRESPONDING GRAPH\n",
        "      df = pd.read_csv(directory+'/'+file, header=None)\n",
        "      df = df.fillna(0)\n",
        "      if directory == uba:\n",
        "        for col in df:\n",
        "          df[col] = df[col].astype(np.int64)\n",
        "      G = nx.from_pandas_adjacency(df)          \n",
        "\n",
        "      # Features\n",
        "      if directory == uba: \n",
        "        if len(feat_dir) == 1:\n",
        "          node_data = pd.read_csv(aug_feat_dir[0][0]+aug_feat_dir[0][1]+str(subject)+'.csv')\n",
        "          node_data = node_data.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "        else: \n",
        "          node_data_1 = pd.read_csv(aug_feat_dir[0][0]+aug_feat_dir[0][1]+str(subject)+'.csv')\n",
        "          node_data_2 = pd.read_csv(aug_feat_dir[1][0]+aug_feat_dir[1][1]+str(subject)+'.csv')\n",
        "          node_data_1 = node_data_1.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "          node_data_2 = node_data_2.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "          node_data = pd.merge(node_data_1, node_data_2, how = \"inner\", left_index=True, right_index=True)\n",
        "      elif directory == uba1:\n",
        "        if len(feat_dir) == 1:\n",
        "          node_data = pd.read_csv(aug_feat_dir1[0][0]+aug_feat_dir1[0][1]+str(subject)+'.csv')\n",
        "          node_data = node_data.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "        else: \n",
        "          node_data_1 = pd.read_csv(aug_feat_dir1[0][0]+aug_feat_dir1[0][1]+str(subject)+'.csv')\n",
        "          node_data_2 = pd.read_csv(aug_feat_dir1[1][0]+aug_feat_dir1[1][1]+str(subject)+'.csv')\n",
        "          node_data_1 = node_data_1.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "          node_data_2 = node_data_2.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "          node_data = pd.merge(node_data_1, node_data_2, how = \"inner\", left_index=True, right_index=True) \n",
        "      else:\n",
        "        if len(feat_dir) == 1:\n",
        "          node_data = pd.read_csv(feat_dir[0][0]+feat_dir[0][1]+str(subject)+'.csv')\n",
        "          node_data = node_data.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "        else:\n",
        "          node_data_1 = pd.read_csv(feat_dir[0][0]+feat_dir[0][1]+str(subject)+'.csv')\n",
        "          node_data_2 = pd.read_csv(feat_dir[1][0]+feat_dir[1][1]+str(subject)+'.csv')\n",
        "          node_data_1 = node_data_1.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "          node_data_2 = node_data_2.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "          node_data = pd.merge(node_data_1, node_data_2, how = \"inner\", left_index=True, right_index=True)\n",
        "\n",
        "      node_data = node_data.drop(['subgraph centrality'], axis=1, errors='ignore')\n",
        "      in_feat = len(node_data.columns)\n",
        "\n",
        "      # Standardisation\n",
        "      cols = list(node_data)\n",
        "      scaler = StandardScaler().fit(node_data)\n",
        "      node_data = scaler.transform(node_data)\n",
        "      node_data = pd.DataFrame(node_data, columns = cols)\n",
        "      \n",
        "      g = StellarGraph.from_networkx(G,node_features=node_data)\n",
        "      graphs.append(g)\n",
        "\n",
        "    #   print(subject)\n",
        "    #   print(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUyZBt78mYP_"
      },
      "outputs": [],
      "source": [
        "graph_labels = pd.DataFrame(graph_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hap8HY-55Jqo"
      },
      "outputs": [],
      "source": [
        "# graph_labels.value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmGfxhMICpMq"
      },
      "outputs": [],
      "source": [
        "graph_labels = pd.get_dummies(graph_labels, drop_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CufcfzeHY82"
      },
      "source": [
        "### DGCNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05KvxfZfC0s1"
      },
      "outputs": [],
      "source": [
        "generator = PaddedGraphGenerator(graphs=graphs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rs_list = [5,20,21,34,40,45,75,69]"
      ],
      "metadata": {
        "id": "F7ci7bbV40BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "acc = []\n",
        "pre = []\n",
        "re = []\n",
        "spe = []\n",
        "f = []\n",
        "es = EarlyStopping(monitor=\"val_acc\", min_delta=0, patience=30, restore_best_weights=True)\n",
        "epochs=100\n",
        "for rs in rs_list:\n",
        "    k = 20 \n",
        "    in_feat = 20\n",
        "    layer_sizes = [in_feat, 32, 64, 128, 128, 64, 32, 16, 4, 2]\n",
        "    dgcnn_model = DeepGraphCNN(\n",
        "        layer_sizes=layer_sizes,\n",
        "        activations=[\"leaky_relu\", \"leaky_relu\", \"leaky_relu\",  \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \n",
        "                        \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"softmax\"],\n",
        "        k=k,\n",
        "        bias=True,\n",
        "        generator=generator,\n",
        "    )\n",
        "    x_inp, x_out = dgcnn_model.in_out_tensors()\n",
        "\n",
        "    x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
        "    x_out = MaxPool1D(pool_size=2)(x_out)\n",
        "    x_out = Conv1D(filters=32, kernel_size=4, strides=1)(x_out)\n",
        "    x_out = Flatten()(x_out)\n",
        "    x_out = Dense(units=128, activation=\"leaky_relu\")(x_out)\n",
        "    x_out = Dropout(rate=0.5)(x_out)\n",
        "\n",
        "    predictions = Dense(units=1, activation=\"sigmoid\")(x_out)\n",
        "\n",
        "    model = Model(inputs=x_inp, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=binary_crossentropy, metrics=[\"acc\"],)\n",
        "\n",
        "    train_graphs, test_graphs = model_selection.train_test_split(graph_labels, train_size=0.8, test_size=0.2, stratify=graph_labels, random_state=rs)\n",
        "    gen = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "    train_gen = gen.flow(\n",
        "        list(train_graphs.index - 1),\n",
        "        targets=train_graphs.values,\n",
        "        batch_size=50,\n",
        "        symmetric_normalization=True,\n",
        "    )\n",
        "\n",
        "    test_gen = gen.flow(\n",
        "        list(test_graphs.index - 1),\n",
        "        targets=test_graphs.values,\n",
        "        batch_size=1,\n",
        "        symmetric_normalization=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True, callbacks=[es],)\n",
        "    test_metrics = model.evaluate(test_gen)\n",
        "\n",
        "    pred = model.predict(test_gen)\n",
        "\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    x = np.array(pred)\n",
        "    for i in x:\n",
        "        if i <0.5:\n",
        "            y_pred.append(0)\n",
        "        else:\n",
        "            y_pred.append(1)\n",
        "\n",
        "    for t in test_gen.targets:\n",
        "        y_true.append(t[0])\n",
        "\n",
        "    f1_score = metrics.f1_score(y_true, y_pred)\n",
        "    precision = metrics.precision_score(y_true, y_pred)\n",
        "    recall =  metrics.recall_score(y_true, y_pred)\n",
        "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(rs)\n",
        "    print(\"Metrics:\")\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"f1 score: \", f1_score)\n",
        "    print(\"precision: \", precision)\n",
        "    print(\"recall: \", recall)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn+fp)\n",
        "    print(\"Specificity\", specificity)\n",
        "\n",
        "    acc.append(accuracy)\n",
        "    f.append(f1_score)\n",
        "    pre.append(precision)\n",
        "    re.append(recall)\n",
        "    spe.append(specificity)\n",
        "\n",
        "    print('\\n\\n')\n",
        "\n",
        "print('Average accccuracy: %.3f +/- %.3f' %(np.mean(acc), np.std(acc)))\n",
        "print('Average precision: %.3f +/- %.3f' %(np.mean(pre), np.std(pre)))\n",
        "print('Average recall: %.3f +/- %.3f' %(np.mean(re), np.std(re)))\n",
        "print('Average f1: %.3f +/- %.3f \\n\\n' %(np.mean(f), np.std(f)))\n",
        "print('Average specificity: %.3f +/- %.3f \\n\\n' %(np.mean(spe), np.std(spe)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhJTDhsViUw2",
        "outputId": "9e120694-f8c7-4710-ae46-d795cd1f4193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 7s 954ms/step - loss: 0.6940 - acc: 0.5051 - val_loss: 0.6947 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 500ms/step - loss: 0.6995 - acc: 0.4796 - val_loss: 0.6922 - val_acc: 0.5800\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 1s 391ms/step - loss: 0.6870 - acc: 0.5510 - val_loss: 0.6921 - val_acc: 0.5400\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 492ms/step - loss: 0.6960 - acc: 0.4745 - val_loss: 0.6919 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 397ms/step - loss: 0.6940 - acc: 0.5051 - val_loss: 0.6900 - val_acc: 0.6000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 503ms/step - loss: 0.6943 - acc: 0.4898 - val_loss: 0.6898 - val_acc: 0.6600\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.6925 - acc: 0.5153 - val_loss: 0.6894 - val_acc: 0.6000\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 492ms/step - loss: 0.6963 - acc: 0.4847 - val_loss: 0.6889 - val_acc: 0.5800\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 1s 387ms/step - loss: 0.6901 - acc: 0.5255 - val_loss: 0.6889 - val_acc: 0.5600\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 1s 396ms/step - loss: 0.6856 - acc: 0.6020 - val_loss: 0.6874 - val_acc: 0.6600\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 1s 396ms/step - loss: 0.6829 - acc: 0.6071 - val_loss: 0.6861 - val_acc: 0.6600\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 400ms/step - loss: 0.6862 - acc: 0.5510 - val_loss: 0.6855 - val_acc: 0.6000\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 497ms/step - loss: 0.6829 - acc: 0.5867 - val_loss: 0.6797 - val_acc: 0.5600\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 397ms/step - loss: 0.6807 - acc: 0.5867 - val_loss: 0.6759 - val_acc: 0.5800\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 506ms/step - loss: 0.6775 - acc: 0.5867 - val_loss: 0.6700 - val_acc: 0.5600\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.6750 - acc: 0.6122 - val_loss: 0.6618 - val_acc: 0.6200\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.6778 - acc: 0.5663 - val_loss: 0.6589 - val_acc: 0.6800\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 396ms/step - loss: 0.6720 - acc: 0.5969 - val_loss: 0.6539 - val_acc: 0.6000\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 1s 383ms/step - loss: 0.6652 - acc: 0.6071 - val_loss: 0.6496 - val_acc: 0.6000\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 1s 381ms/step - loss: 0.6525 - acc: 0.6173 - val_loss: 0.6465 - val_acc: 0.6400\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 1s 387ms/step - loss: 0.6433 - acc: 0.6327 - val_loss: 0.6206 - val_acc: 0.6000\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 1s 393ms/step - loss: 0.6374 - acc: 0.6378 - val_loss: 0.6186 - val_acc: 0.5800\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 1s 398ms/step - loss: 0.6313 - acc: 0.6224 - val_loss: 0.6069 - val_acc: 0.6600\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 498ms/step - loss: 0.6183 - acc: 0.6633 - val_loss: 0.5985 - val_acc: 0.6800\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 406ms/step - loss: 0.6076 - acc: 0.6735 - val_loss: 0.5857 - val_acc: 0.6600\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 1s 396ms/step - loss: 0.6194 - acc: 0.6531 - val_loss: 0.5911 - val_acc: 0.6200\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.5951 - acc: 0.6888 - val_loss: 0.5824 - val_acc: 0.6800\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.5991 - acc: 0.6582 - val_loss: 0.5874 - val_acc: 0.6800\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 3s 686ms/step - loss: 0.6137 - acc: 0.6429 - val_loss: 0.5754 - val_acc: 0.6400\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.5731 - acc: 0.7092 - val_loss: 0.5767 - val_acc: 0.6400\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 489ms/step - loss: 0.5773 - acc: 0.6888 - val_loss: 0.5771 - val_acc: 0.7200\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 513ms/step - loss: 0.5811 - acc: 0.6735 - val_loss: 0.5745 - val_acc: 0.7200\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 1s 389ms/step - loss: 0.5653 - acc: 0.6837 - val_loss: 0.5600 - val_acc: 0.7000\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.5541 - acc: 0.7143 - val_loss: 0.5735 - val_acc: 0.6600\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.5779 - acc: 0.6735 - val_loss: 0.5769 - val_acc: 0.6800\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 1s 393ms/step - loss: 0.5681 - acc: 0.6939 - val_loss: 0.5385 - val_acc: 0.7200\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.5710 - acc: 0.6939 - val_loss: 0.5680 - val_acc: 0.6400\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 0.5501 - acc: 0.7143 - val_loss: 0.5585 - val_acc: 0.7000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 499ms/step - loss: 0.5320 - acc: 0.7245 - val_loss: 0.5940 - val_acc: 0.6800\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 1s 403ms/step - loss: 0.5560 - acc: 0.6735 - val_loss: 0.5372 - val_acc: 0.7000\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.5296 - acc: 0.7041 - val_loss: 0.5425 - val_acc: 0.7000\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 1s 400ms/step - loss: 0.5481 - acc: 0.6786 - val_loss: 0.5757 - val_acc: 0.6200\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.5356 - acc: 0.7092 - val_loss: 0.5295 - val_acc: 0.6800\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.5151 - acc: 0.7500 - val_loss: 0.5326 - val_acc: 0.6800\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 2s 395ms/step - loss: 0.5065 - acc: 0.7551 - val_loss: 0.5415 - val_acc: 0.7400\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 502ms/step - loss: 0.4964 - acc: 0.7449 - val_loss: 0.5379 - val_acc: 0.6600\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.4929 - acc: 0.7704 - val_loss: 0.5158 - val_acc: 0.7200\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 394ms/step - loss: 0.4713 - acc: 0.7653 - val_loss: 0.5257 - val_acc: 0.7400\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 1s 397ms/step - loss: 0.4702 - acc: 0.7653 - val_loss: 0.5216 - val_acc: 0.6800\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.4595 - acc: 0.7653 - val_loss: 0.5214 - val_acc: 0.7000\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.4769 - acc: 0.7602 - val_loss: 0.5291 - val_acc: 0.6800\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 1s 391ms/step - loss: 0.4799 - acc: 0.7551 - val_loss: 0.5292 - val_acc: 0.6600\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 1s 392ms/step - loss: 0.4677 - acc: 0.7755 - val_loss: 0.5765 - val_acc: 0.6600\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 1s 392ms/step - loss: 0.4542 - acc: 0.7857 - val_loss: 0.5480 - val_acc: 0.6800\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 1s 386ms/step - loss: 0.4643 - acc: 0.7500 - val_loss: 0.5538 - val_acc: 0.6400\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 2s 504ms/step - loss: 0.4556 - acc: 0.7704 - val_loss: 0.5195 - val_acc: 0.7200\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 3s 729ms/step - loss: 0.4445 - acc: 0.7704 - val_loss: 0.5535 - val_acc: 0.6800\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.4517 - acc: 0.7653 - val_loss: 0.5608 - val_acc: 0.6600\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 2s 497ms/step - loss: 0.4517 - acc: 0.7500 - val_loss: 0.5381 - val_acc: 0.7600\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 2s 392ms/step - loss: 0.4385 - acc: 0.7653 - val_loss: 0.5522 - val_acc: 0.6600\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 2s 515ms/step - loss: 0.4434 - acc: 0.8010 - val_loss: 0.5488 - val_acc: 0.7800\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 1s 382ms/step - loss: 0.4475 - acc: 0.7806 - val_loss: 0.5439 - val_acc: 0.7000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.4431 - acc: 0.7908 - val_loss: 0.5292 - val_acc: 0.7200\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 2s 392ms/step - loss: 0.4160 - acc: 0.7908 - val_loss: 0.5382 - val_acc: 0.7000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 2s 414ms/step - loss: 0.4011 - acc: 0.8265 - val_loss: 0.4889 - val_acc: 0.7400\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 2s 497ms/step - loss: 0.4093 - acc: 0.7908 - val_loss: 0.5442 - val_acc: 0.7200\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.4184 - acc: 0.7959 - val_loss: 0.5121 - val_acc: 0.7400\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.4440 - acc: 0.7908 - val_loss: 0.5111 - val_acc: 0.7000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.4456 - acc: 0.7857 - val_loss: 0.5350 - val_acc: 0.7000\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.4492 - acc: 0.7806 - val_loss: 0.5169 - val_acc: 0.7200\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 2s 395ms/step - loss: 0.4107 - acc: 0.7755 - val_loss: 0.5200 - val_acc: 0.7000\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 2s 500ms/step - loss: 0.4450 - acc: 0.7398 - val_loss: 0.5147 - val_acc: 0.7000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 2s 396ms/step - loss: 0.4270 - acc: 0.7857 - val_loss: 0.5089 - val_acc: 0.7000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 1s 390ms/step - loss: 0.4158 - acc: 0.7857 - val_loss: 0.5065 - val_acc: 0.7000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 2s 497ms/step - loss: 0.4137 - acc: 0.7857 - val_loss: 0.5126 - val_acc: 0.7200\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 1s 391ms/step - loss: 0.4358 - acc: 0.7959 - val_loss: 0.5033 - val_acc: 0.7600\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 2s 411ms/step - loss: 0.4153 - acc: 0.7857 - val_loss: 0.5161 - val_acc: 0.6800\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 2s 493ms/step - loss: 0.4364 - acc: 0.7959 - val_loss: 0.5093 - val_acc: 0.7000\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 2s 501ms/step - loss: 0.4214 - acc: 0.7806 - val_loss: 0.5556 - val_acc: 0.7800\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 2s 499ms/step - loss: 0.4257 - acc: 0.8061 - val_loss: 0.5208 - val_acc: 0.7000\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 2s 622ms/step - loss: 0.4168 - acc: 0.7959 - val_loss: 0.4783 - val_acc: 0.8000\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 3s 662ms/step - loss: 0.3833 - acc: 0.8265 - val_loss: 0.4608 - val_acc: 0.7600\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 2s 493ms/step - loss: 0.3859 - acc: 0.8265 - val_loss: 0.4496 - val_acc: 0.7200\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.3718 - acc: 0.8469 - val_loss: 0.4715 - val_acc: 0.7400\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 2s 496ms/step - loss: 0.3543 - acc: 0.8469 - val_loss: 0.4769 - val_acc: 0.7600\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 1s 385ms/step - loss: 0.3739 - acc: 0.8367 - val_loss: 0.5156 - val_acc: 0.7400\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.3748 - acc: 0.8163 - val_loss: 0.5262 - val_acc: 0.7200\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 0.3747 - acc: 0.8163 - val_loss: 0.5255 - val_acc: 0.7600\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 1s 393ms/step - loss: 0.3658 - acc: 0.8316 - val_loss: 0.5157 - val_acc: 0.7600\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 0.3330 - acc: 0.8469 - val_loss: 0.5056 - val_acc: 0.8000\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 2s 391ms/step - loss: 0.3567 - acc: 0.8418 - val_loss: 0.5423 - val_acc: 0.7400\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.3713 - acc: 0.8316 - val_loss: 0.5656 - val_acc: 0.7200\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 1s 389ms/step - loss: 0.3651 - acc: 0.8367 - val_loss: 0.5045 - val_acc: 0.7800\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 1s 387ms/step - loss: 0.3389 - acc: 0.8571 - val_loss: 0.5260 - val_acc: 0.7400\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.3356 - acc: 0.8520 - val_loss: 0.4948 - val_acc: 0.7600\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 2s 396ms/step - loss: 0.3460 - acc: 0.8571 - val_loss: 0.4952 - val_acc: 0.7800\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 2s 394ms/step - loss: 0.3227 - acc: 0.8622 - val_loss: 0.4924 - val_acc: 0.7600\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.3271 - acc: 0.8520 - val_loss: 0.4984 - val_acc: 0.7800\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 2s 394ms/step - loss: 0.3212 - acc: 0.8776 - val_loss: 0.5009 - val_acc: 0.7800\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 1s 391ms/step - loss: 0.3082 - acc: 0.8776 - val_loss: 0.4961 - val_acc: 0.8000\n",
            "50/50 [==============================] - 0s 7ms/step - loss: 0.4961 - acc: 0.8000\n",
            "50/50 [==============================] - 1s 7ms/step\n",
            "5\n",
            "Metrics:\n",
            "Accuracy:  0.8\n",
            "f1 score:  0.8000000000000002\n",
            "precision:  0.8\n",
            "recall:  0.8\n",
            "Specificity 0.8\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 5s 618ms/step - loss: 0.7001 - acc: 0.4796 - val_loss: 0.6953 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.6921 - acc: 0.4847 - val_loss: 0.6920 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 418ms/step - loss: 0.6959 - acc: 0.4745 - val_loss: 0.6919 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.6961 - acc: 0.4643 - val_loss: 0.6908 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 506ms/step - loss: 0.6901 - acc: 0.5357 - val_loss: 0.6906 - val_acc: 0.6200\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 545ms/step - loss: 0.6914 - acc: 0.5561 - val_loss: 0.6875 - val_acc: 0.7000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 3s 713ms/step - loss: 0.6891 - acc: 0.5357 - val_loss: 0.6879 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.6926 - acc: 0.5408 - val_loss: 0.6862 - val_acc: 0.7000\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.6956 - acc: 0.4949 - val_loss: 0.6861 - val_acc: 0.6800\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.6828 - acc: 0.5357 - val_loss: 0.6847 - val_acc: 0.6000\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.6906 - acc: 0.5561 - val_loss: 0.6801 - val_acc: 0.5800\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.6890 - acc: 0.5102 - val_loss: 0.6770 - val_acc: 0.7400\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.6880 - acc: 0.5459 - val_loss: 0.6743 - val_acc: 0.6200\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 509ms/step - loss: 0.6871 - acc: 0.5561 - val_loss: 0.6702 - val_acc: 0.6600\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 433ms/step - loss: 0.6834 - acc: 0.5663 - val_loss: 0.6688 - val_acc: 0.5400\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.6799 - acc: 0.5918 - val_loss: 0.6555 - val_acc: 0.7000\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 518ms/step - loss: 0.6744 - acc: 0.6224 - val_loss: 0.6488 - val_acc: 0.7800\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 397ms/step - loss: 0.6677 - acc: 0.5816 - val_loss: 0.6424 - val_acc: 0.7200\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.6555 - acc: 0.6276 - val_loss: 0.6274 - val_acc: 0.7400\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.6636 - acc: 0.5969 - val_loss: 0.6144 - val_acc: 0.7800\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 416ms/step - loss: 0.6410 - acc: 0.6531 - val_loss: 0.6030 - val_acc: 0.7200\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.6358 - acc: 0.6224 - val_loss: 0.5902 - val_acc: 0.7600\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 521ms/step - loss: 0.6458 - acc: 0.6071 - val_loss: 0.5709 - val_acc: 0.7400\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.6464 - acc: 0.6276 - val_loss: 0.5597 - val_acc: 0.7400\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.6601 - acc: 0.5969 - val_loss: 0.5985 - val_acc: 0.7200\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.6035 - acc: 0.6837 - val_loss: 0.5977 - val_acc: 0.6400\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 394ms/step - loss: 0.6359 - acc: 0.6020 - val_loss: 0.5517 - val_acc: 0.7000\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 502ms/step - loss: 0.6286 - acc: 0.6429 - val_loss: 0.5921 - val_acc: 0.7200\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.6147 - acc: 0.6735 - val_loss: 0.5595 - val_acc: 0.7000\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.5909 - acc: 0.7041 - val_loss: 0.5478 - val_acc: 0.7200\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 3s 701ms/step - loss: 0.5947 - acc: 0.6786 - val_loss: 0.5175 - val_acc: 0.7400\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 422ms/step - loss: 0.5689 - acc: 0.6786 - val_loss: 0.5453 - val_acc: 0.7200\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.5730 - acc: 0.6837 - val_loss: 0.5098 - val_acc: 0.7600\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.5725 - acc: 0.6582 - val_loss: 0.4893 - val_acc: 0.7800\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.5853 - acc: 0.6480 - val_loss: 0.4893 - val_acc: 0.7200\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 400ms/step - loss: 0.6172 - acc: 0.6735 - val_loss: 0.4906 - val_acc: 0.7800\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 503ms/step - loss: 0.6298 - acc: 0.6327 - val_loss: 0.5728 - val_acc: 0.7200\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.6042 - acc: 0.6684 - val_loss: 0.5288 - val_acc: 0.7400\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 509ms/step - loss: 0.5917 - acc: 0.6939 - val_loss: 0.4977 - val_acc: 0.7800\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.5732 - acc: 0.6786 - val_loss: 0.5150 - val_acc: 0.7000\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 394ms/step - loss: 0.5634 - acc: 0.7041 - val_loss: 0.4981 - val_acc: 0.7400\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.5514 - acc: 0.6990 - val_loss: 0.5020 - val_acc: 0.7400\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.5545 - acc: 0.7092 - val_loss: 0.4913 - val_acc: 0.7400\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.5547 - acc: 0.6990 - val_loss: 0.4774 - val_acc: 0.7200\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.5322 - acc: 0.7092 - val_loss: 0.4811 - val_acc: 0.7600\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.5321 - acc: 0.6990 - val_loss: 0.4766 - val_acc: 0.7000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.5325 - acc: 0.7041 - val_loss: 0.4719 - val_acc: 0.7400\n",
            "50/50 [==============================] - 0s 7ms/step - loss: 0.6488 - acc: 0.7800\n",
            "50/50 [==============================] - 1s 7ms/step\n",
            "20\n",
            "Metrics:\n",
            "Accuracy:  0.78\n",
            "f1 score:  0.7843137254901961\n",
            "precision:  0.7692307692307693\n",
            "recall:  0.8\n",
            "Specificity 0.76\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 5s 740ms/step - loss: 0.6914 - acc: 0.5000 - val_loss: 0.6934 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 408ms/step - loss: 0.6930 - acc: 0.5153 - val_loss: 0.6941 - val_acc: 0.4200\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 511ms/step - loss: 0.7020 - acc: 0.4745 - val_loss: 0.6954 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 417ms/step - loss: 0.6939 - acc: 0.5204 - val_loss: 0.6950 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.6832 - acc: 0.5561 - val_loss: 0.6948 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.6911 - acc: 0.5561 - val_loss: 0.6959 - val_acc: 0.4800\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 411ms/step - loss: 0.6852 - acc: 0.5408 - val_loss: 0.6932 - val_acc: 0.5400\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 3s 762ms/step - loss: 0.6957 - acc: 0.5612 - val_loss: 0.7003 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.6890 - acc: 0.5204 - val_loss: 0.6952 - val_acc: 0.4800\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.6886 - acc: 0.5357 - val_loss: 0.6946 - val_acc: 0.4800\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.6807 - acc: 0.5663 - val_loss: 0.6943 - val_acc: 0.5200\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6713 - acc: 0.6327 - val_loss: 0.6955 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6805 - acc: 0.5765 - val_loss: 0.6986 - val_acc: 0.4800\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 493ms/step - loss: 0.6795 - acc: 0.5663 - val_loss: 0.6898 - val_acc: 0.5400\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 492ms/step - loss: 0.6826 - acc: 0.5663 - val_loss: 0.6953 - val_acc: 0.4800\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 506ms/step - loss: 0.6888 - acc: 0.5357 - val_loss: 0.6963 - val_acc: 0.4800\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 1s 395ms/step - loss: 0.6726 - acc: 0.5612 - val_loss: 0.6844 - val_acc: 0.5000\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 1s 391ms/step - loss: 0.6749 - acc: 0.5663 - val_loss: 0.6743 - val_acc: 0.5600\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 0.6824 - acc: 0.5867 - val_loss: 0.6758 - val_acc: 0.6000\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 435ms/step - loss: 0.6490 - acc: 0.6224 - val_loss: 0.6351 - val_acc: 0.6200\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 416ms/step - loss: 0.6544 - acc: 0.6327 - val_loss: 0.6213 - val_acc: 0.7000\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.6542 - acc: 0.6173 - val_loss: 0.6198 - val_acc: 0.6800\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.6463 - acc: 0.6327 - val_loss: 0.5858 - val_acc: 0.7000\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.6225 - acc: 0.6837 - val_loss: 0.5616 - val_acc: 0.6800\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.6123 - acc: 0.6684 - val_loss: 0.5513 - val_acc: 0.6600\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.6236 - acc: 0.6327 - val_loss: 0.5328 - val_acc: 0.6800\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 503ms/step - loss: 0.5666 - acc: 0.7143 - val_loss: 0.5050 - val_acc: 0.7400\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.5764 - acc: 0.7143 - val_loss: 0.5145 - val_acc: 0.6800\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 511ms/step - loss: 0.5664 - acc: 0.6786 - val_loss: 0.5344 - val_acc: 0.6600\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.5345 - acc: 0.7143 - val_loss: 0.5789 - val_acc: 0.6800\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.5663 - acc: 0.6990 - val_loss: 0.5704 - val_acc: 0.7000\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 395ms/step - loss: 0.5606 - acc: 0.7398 - val_loss: 0.6398 - val_acc: 0.6400\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 492ms/step - loss: 0.6263 - acc: 0.6735 - val_loss: 0.5224 - val_acc: 0.7000\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 414ms/step - loss: 0.5959 - acc: 0.6582 - val_loss: 0.5439 - val_acc: 0.7400\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 395ms/step - loss: 0.5550 - acc: 0.7194 - val_loss: 0.5201 - val_acc: 0.7000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 3s 731ms/step - loss: 0.5343 - acc: 0.7500 - val_loss: 0.5503 - val_acc: 0.7000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 542ms/step - loss: 0.5337 - acc: 0.7143 - val_loss: 0.5228 - val_acc: 0.6800\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.5020 - acc: 0.7704 - val_loss: 0.5460 - val_acc: 0.6800\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 423ms/step - loss: 0.5332 - acc: 0.7245 - val_loss: 0.5058 - val_acc: 0.6800\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 427ms/step - loss: 0.4947 - acc: 0.7602 - val_loss: 0.5322 - val_acc: 0.7200\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 420ms/step - loss: 0.5133 - acc: 0.7245 - val_loss: 0.4974 - val_acc: 0.6400\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.5183 - acc: 0.7296 - val_loss: 0.5091 - val_acc: 0.7400\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.4960 - acc: 0.7347 - val_loss: 0.5095 - val_acc: 0.6600\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.4959 - acc: 0.7347 - val_loss: 0.5263 - val_acc: 0.7200\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 2s 448ms/step - loss: 0.4957 - acc: 0.7347 - val_loss: 0.5068 - val_acc: 0.7400\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 539ms/step - loss: 0.4700 - acc: 0.7500 - val_loss: 0.4941 - val_acc: 0.6800\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 541ms/step - loss: 0.4618 - acc: 0.7755 - val_loss: 0.4887 - val_acc: 0.6800\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 427ms/step - loss: 0.4856 - acc: 0.7755 - val_loss: 0.4739 - val_acc: 0.7000\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 2s 453ms/step - loss: 0.5117 - acc: 0.7398 - val_loss: 0.4781 - val_acc: 0.6800\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.4591 - acc: 0.7653 - val_loss: 0.5627 - val_acc: 0.7000\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 2s 521ms/step - loss: 0.4965 - acc: 0.7500 - val_loss: 0.5133 - val_acc: 0.7000\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 2s 537ms/step - loss: 0.5155 - acc: 0.7194 - val_loss: 0.4960 - val_acc: 0.6800\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.4773 - acc: 0.7500 - val_loss: 0.4816 - val_acc: 0.7000\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.4937 - acc: 0.7143 - val_loss: 0.5358 - val_acc: 0.7200\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.4757 - acc: 0.7500 - val_loss: 0.4966 - val_acc: 0.7200\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.4658 - acc: 0.7653 - val_loss: 0.5299 - val_acc: 0.7400\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.4901 - acc: 0.7449 - val_loss: 0.4769 - val_acc: 0.7200\n",
            "50/50 [==============================] - 0s 7ms/step - loss: 0.5050 - acc: 0.7400\n",
            "50/50 [==============================] - 1s 7ms/step\n",
            "21\n",
            "Metrics:\n",
            "Accuracy:  0.74\n",
            "f1 score:  0.7450980392156863\n",
            "precision:  0.7307692307692307\n",
            "recall:  0.76\n",
            "Specificity 0.72\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 7s 654ms/step - loss: 0.6959 - acc: 0.4388 - val_loss: 0.6926 - val_acc: 0.5600\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 427ms/step - loss: 0.6916 - acc: 0.5204 - val_loss: 0.6928 - val_acc: 0.4800\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 444ms/step - loss: 0.6944 - acc: 0.4898 - val_loss: 0.6923 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 434ms/step - loss: 0.6969 - acc: 0.4796 - val_loss: 0.6906 - val_acc: 0.6000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.6967 - acc: 0.5051 - val_loss: 0.6906 - val_acc: 0.6400\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 540ms/step - loss: 0.6882 - acc: 0.5612 - val_loss: 0.6883 - val_acc: 0.5400\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 538ms/step - loss: 0.6873 - acc: 0.5459 - val_loss: 0.6870 - val_acc: 0.5200\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 535ms/step - loss: 0.6804 - acc: 0.5969 - val_loss: 0.6841 - val_acc: 0.5600\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 449ms/step - loss: 0.6799 - acc: 0.5867 - val_loss: 0.6816 - val_acc: 0.6000\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.6732 - acc: 0.6276 - val_loss: 0.6765 - val_acc: 0.6200\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.6686 - acc: 0.6071 - val_loss: 0.6752 - val_acc: 0.5400\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 535ms/step - loss: 0.6633 - acc: 0.5918 - val_loss: 0.6706 - val_acc: 0.6200\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 437ms/step - loss: 0.6440 - acc: 0.6122 - val_loss: 0.6734 - val_acc: 0.5400\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 434ms/step - loss: 0.6466 - acc: 0.6173 - val_loss: 0.6614 - val_acc: 0.5800\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.6179 - acc: 0.6633 - val_loss: 0.6524 - val_acc: 0.6200\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 424ms/step - loss: 0.6066 - acc: 0.6888 - val_loss: 0.6465 - val_acc: 0.5800\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.5999 - acc: 0.6786 - val_loss: 0.6217 - val_acc: 0.6600\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.5840 - acc: 0.6939 - val_loss: 0.6069 - val_acc: 0.6200\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.5877 - acc: 0.6837 - val_loss: 0.6046 - val_acc: 0.6600\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.5549 - acc: 0.6990 - val_loss: 0.5869 - val_acc: 0.6600\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.5548 - acc: 0.7194 - val_loss: 0.5774 - val_acc: 0.6800\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 402ms/step - loss: 0.5492 - acc: 0.7296 - val_loss: 0.5604 - val_acc: 0.7200\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 501ms/step - loss: 0.5320 - acc: 0.7398 - val_loss: 0.5582 - val_acc: 0.7000\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.5327 - acc: 0.7398 - val_loss: 0.5418 - val_acc: 0.7400\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 504ms/step - loss: 0.5313 - acc: 0.7296 - val_loss: 0.5336 - val_acc: 0.7400\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 3s 740ms/step - loss: 0.5355 - acc: 0.7398 - val_loss: 0.5271 - val_acc: 0.7400\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.5260 - acc: 0.7449 - val_loss: 0.5114 - val_acc: 0.7800\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.5223 - acc: 0.7500 - val_loss: 0.5126 - val_acc: 0.7000\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 541ms/step - loss: 0.5045 - acc: 0.7653 - val_loss: 0.5075 - val_acc: 0.7200\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 452ms/step - loss: 0.4966 - acc: 0.7500 - val_loss: 0.4900 - val_acc: 0.7600\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 441ms/step - loss: 0.4914 - acc: 0.7551 - val_loss: 0.4890 - val_acc: 0.7800\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.4815 - acc: 0.7602 - val_loss: 0.4943 - val_acc: 0.7200\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 532ms/step - loss: 0.4952 - acc: 0.7602 - val_loss: 0.4760 - val_acc: 0.8000\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.5176 - acc: 0.7296 - val_loss: 0.5475 - val_acc: 0.7600\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 536ms/step - loss: 0.4965 - acc: 0.7551 - val_loss: 0.4860 - val_acc: 0.7200\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.4813 - acc: 0.7857 - val_loss: 0.5365 - val_acc: 0.7200\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 435ms/step - loss: 0.4977 - acc: 0.7449 - val_loss: 0.4874 - val_acc: 0.7400\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 0.4792 - acc: 0.7857 - val_loss: 0.5058 - val_acc: 0.7200\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.5017 - acc: 0.7449 - val_loss: 0.5118 - val_acc: 0.7600\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.4706 - acc: 0.7806 - val_loss: 0.4804 - val_acc: 0.7400\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.4716 - acc: 0.7857 - val_loss: 0.4780 - val_acc: 0.7800\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.4585 - acc: 0.8061 - val_loss: 0.4820 - val_acc: 0.7600\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 402ms/step - loss: 0.4525 - acc: 0.7908 - val_loss: 0.4934 - val_acc: 0.7400\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 396ms/step - loss: 0.4444 - acc: 0.8061 - val_loss: 0.4713 - val_acc: 0.7600\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.4405 - acc: 0.8163 - val_loss: 0.4609 - val_acc: 0.8200\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.4385 - acc: 0.8112 - val_loss: 0.4655 - val_acc: 0.7800\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.4339 - acc: 0.8010 - val_loss: 0.4668 - val_acc: 0.7600\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 400ms/step - loss: 0.4204 - acc: 0.8112 - val_loss: 0.4574 - val_acc: 0.8000\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.4299 - acc: 0.8163 - val_loss: 0.4985 - val_acc: 0.7600\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 515ms/step - loss: 0.4265 - acc: 0.8010 - val_loss: 0.4731 - val_acc: 0.7800\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 3s 726ms/step - loss: 0.4255 - acc: 0.7806 - val_loss: 0.5069 - val_acc: 0.7600\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.4344 - acc: 0.7755 - val_loss: 0.5768 - val_acc: 0.7800\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 1s 387ms/step - loss: 0.4281 - acc: 0.7857 - val_loss: 0.6725 - val_acc: 0.7600\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 2s 495ms/step - loss: 0.4388 - acc: 0.7908 - val_loss: 0.6696 - val_acc: 0.7600\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.4511 - acc: 0.7857 - val_loss: 0.6397 - val_acc: 0.7800\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.4722 - acc: 0.7551 - val_loss: 0.6100 - val_acc: 0.7400\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 2s 432ms/step - loss: 0.4848 - acc: 0.7704 - val_loss: 0.5965 - val_acc: 0.7600\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.5014 - acc: 0.7704 - val_loss: 0.5839 - val_acc: 0.6800\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.4896 - acc: 0.7806 - val_loss: 0.5308 - val_acc: 0.7200\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.4746 - acc: 0.7653 - val_loss: 0.5302 - val_acc: 0.7000\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 2s 537ms/step - loss: 0.4880 - acc: 0.7602 - val_loss: 0.5689 - val_acc: 0.7600\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.4804 - acc: 0.7908 - val_loss: 0.6013 - val_acc: 0.7000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.4794 - acc: 0.7602 - val_loss: 0.6446 - val_acc: 0.7000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 2s 423ms/step - loss: 0.4883 - acc: 0.7857 - val_loss: 0.5852 - val_acc: 0.7200\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.4768 - acc: 0.7959 - val_loss: 0.5721 - val_acc: 0.7000\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.4740 - acc: 0.8010 - val_loss: 0.5766 - val_acc: 0.7000\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.4675 - acc: 0.7755 - val_loss: 0.5759 - val_acc: 0.6800\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.4563 - acc: 0.7959 - val_loss: 0.5798 - val_acc: 0.6800\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.4763 - acc: 0.7857 - val_loss: 0.5833 - val_acc: 0.6800\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.4657 - acc: 0.7857 - val_loss: 0.6060 - val_acc: 0.6800\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 2s 408ms/step - loss: 0.4871 - acc: 0.7755 - val_loss: 0.6061 - val_acc: 0.7200\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 2s 499ms/step - loss: 0.4651 - acc: 0.7806 - val_loss: 0.6686 - val_acc: 0.6800\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 2s 414ms/step - loss: 0.4941 - acc: 0.7449 - val_loss: 0.5659 - val_acc: 0.6600\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 2s 404ms/step - loss: 0.4708 - acc: 0.7653 - val_loss: 0.5255 - val_acc: 0.7200\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 2s 501ms/step - loss: 0.4719 - acc: 0.7806 - val_loss: 0.5270 - val_acc: 0.7000\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.4609 - acc: 0.8200\n",
            "50/50 [==============================] - 1s 7ms/step\n",
            "34\n",
            "Metrics:\n",
            "Accuracy:  0.82\n",
            "f1 score:  0.816326530612245\n",
            "precision:  0.8333333333333334\n",
            "recall:  0.8\n",
            "Specificity 0.84\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 7s 884ms/step - loss: 0.6981 - acc: 0.4898 - val_loss: 0.6917 - val_acc: 0.5200\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.6918 - acc: 0.5000 - val_loss: 0.6912 - val_acc: 0.5400\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 1s 388ms/step - loss: 0.6944 - acc: 0.5255 - val_loss: 0.6932 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 496ms/step - loss: 0.6883 - acc: 0.5357 - val_loss: 0.6931 - val_acc: 0.5200\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.6972 - acc: 0.4286 - val_loss: 0.6929 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 434ms/step - loss: 0.6889 - acc: 0.5561 - val_loss: 0.6939 - val_acc: 0.4800\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 446ms/step - loss: 0.6861 - acc: 0.5561 - val_loss: 0.6901 - val_acc: 0.5600\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 447ms/step - loss: 0.6849 - acc: 0.5663 - val_loss: 0.6880 - val_acc: 0.6200\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 515ms/step - loss: 0.6874 - acc: 0.5612 - val_loss: 0.6804 - val_acc: 0.6400\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6805 - acc: 0.6071 - val_loss: 0.6724 - val_acc: 0.7400\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.6776 - acc: 0.5969 - val_loss: 0.6694 - val_acc: 0.7400\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.6688 - acc: 0.6071 - val_loss: 0.6615 - val_acc: 0.7400\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.6678 - acc: 0.5714 - val_loss: 0.6696 - val_acc: 0.6000\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 439ms/step - loss: 0.6718 - acc: 0.6224 - val_loss: 0.6551 - val_acc: 0.7200\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 416ms/step - loss: 0.6372 - acc: 0.6582 - val_loss: 0.6691 - val_acc: 0.5400\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 0.6443 - acc: 0.6122 - val_loss: 0.6616 - val_acc: 0.6400\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 432ms/step - loss: 0.6343 - acc: 0.6531 - val_loss: 0.6540 - val_acc: 0.5800\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6137 - acc: 0.6276 - val_loss: 0.6164 - val_acc: 0.6600\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 426ms/step - loss: 0.5955 - acc: 0.6735 - val_loss: 0.6380 - val_acc: 0.6400\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 511ms/step - loss: 0.6002 - acc: 0.6582 - val_loss: 0.7472 - val_acc: 0.6000\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 420ms/step - loss: 0.6236 - acc: 0.6378 - val_loss: 0.6069 - val_acc: 0.6400\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.6008 - acc: 0.6480 - val_loss: 0.6518 - val_acc: 0.6000\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.5911 - acc: 0.6888 - val_loss: 0.6469 - val_acc: 0.6600\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 408ms/step - loss: 0.5876 - acc: 0.6531 - val_loss: 0.6757 - val_acc: 0.6200\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 644ms/step - loss: 0.5776 - acc: 0.6735 - val_loss: 0.6490 - val_acc: 0.6400\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 3s 684ms/step - loss: 0.5871 - acc: 0.6888 - val_loss: 0.6556 - val_acc: 0.6000\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 500ms/step - loss: 0.5658 - acc: 0.7092 - val_loss: 0.6593 - val_acc: 0.6200\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 495ms/step - loss: 0.5656 - acc: 0.7041 - val_loss: 0.6644 - val_acc: 0.6400\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 503ms/step - loss: 0.5394 - acc: 0.7041 - val_loss: 0.6681 - val_acc: 0.6400\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 0.5408 - acc: 0.6990 - val_loss: 0.6834 - val_acc: 0.6200\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 1s 395ms/step - loss: 0.5482 - acc: 0.6990 - val_loss: 0.6911 - val_acc: 0.6000\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 417ms/step - loss: 0.5302 - acc: 0.7347 - val_loss: 0.6807 - val_acc: 0.6000\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.5408 - acc: 0.7398 - val_loss: 0.6700 - val_acc: 0.6400\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 457ms/step - loss: 0.5279 - acc: 0.7551 - val_loss: 0.6627 - val_acc: 0.5800\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 430ms/step - loss: 0.5351 - acc: 0.7041 - val_loss: 0.6953 - val_acc: 0.6000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.5308 - acc: 0.7500 - val_loss: 0.7357 - val_acc: 0.5600\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 434ms/step - loss: 0.5306 - acc: 0.7449 - val_loss: 0.7150 - val_acc: 0.6000\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.5099 - acc: 0.7347 - val_loss: 0.6828 - val_acc: 0.6400\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.5135 - acc: 0.7602 - val_loss: 0.6904 - val_acc: 0.6000\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 546ms/step - loss: 0.5173 - acc: 0.7398 - val_loss: 0.6724 - val_acc: 0.6400\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.6724 - acc: 0.7400\n",
            "50/50 [==============================] - 1s 8ms/step\n",
            "55\n",
            "Metrics:\n",
            "Accuracy:  0.74\n",
            "f1 score:  0.7450980392156863\n",
            "precision:  0.7307692307692307\n",
            "recall:  0.76\n",
            "Specificity 0.72\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 6s 780ms/step - loss: 0.6952 - acc: 0.5000 - val_loss: 0.6923 - val_acc: 0.6000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 505ms/step - loss: 0.6917 - acc: 0.5561 - val_loss: 0.6930 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 420ms/step - loss: 0.6937 - acc: 0.4898 - val_loss: 0.6933 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 419ms/step - loss: 0.6936 - acc: 0.4949 - val_loss: 0.6922 - val_acc: 0.5200\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 513ms/step - loss: 0.6958 - acc: 0.5357 - val_loss: 0.6923 - val_acc: 0.5800\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 446ms/step - loss: 0.6921 - acc: 0.5153 - val_loss: 0.6914 - val_acc: 0.4800\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 585ms/step - loss: 0.6885 - acc: 0.5153 - val_loss: 0.6936 - val_acc: 0.4800\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 3s 702ms/step - loss: 0.6879 - acc: 0.5408 - val_loss: 0.6917 - val_acc: 0.4800\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 396ms/step - loss: 0.6881 - acc: 0.5102 - val_loss: 0.6924 - val_acc: 0.4600\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 495ms/step - loss: 0.6878 - acc: 0.5357 - val_loss: 0.6923 - val_acc: 0.6200\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.6879 - acc: 0.4949 - val_loss: 0.6919 - val_acc: 0.6400\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 498ms/step - loss: 0.6843 - acc: 0.5459 - val_loss: 0.6910 - val_acc: 0.5800\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 498ms/step - loss: 0.6852 - acc: 0.5714 - val_loss: 0.6936 - val_acc: 0.6200\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 400ms/step - loss: 0.6901 - acc: 0.5051 - val_loss: 0.6846 - val_acc: 0.6200\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.6916 - acc: 0.5459 - val_loss: 0.6958 - val_acc: 0.5600\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6787 - acc: 0.5663 - val_loss: 0.6880 - val_acc: 0.6200\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6683 - acc: 0.5918 - val_loss: 0.6815 - val_acc: 0.5600\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.6726 - acc: 0.5867 - val_loss: 0.6771 - val_acc: 0.5800\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 424ms/step - loss: 0.6663 - acc: 0.5918 - val_loss: 0.6679 - val_acc: 0.6600\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 417ms/step - loss: 0.6632 - acc: 0.6173 - val_loss: 0.6590 - val_acc: 0.6000\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 400ms/step - loss: 0.6495 - acc: 0.6071 - val_loss: 0.6513 - val_acc: 0.6600\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6484 - acc: 0.6224 - val_loss: 0.6573 - val_acc: 0.6000\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 511ms/step - loss: 0.6253 - acc: 0.6480 - val_loss: 0.6323 - val_acc: 0.6800\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6345 - acc: 0.6173 - val_loss: 0.6251 - val_acc: 0.6400\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.6094 - acc: 0.6531 - val_loss: 0.6164 - val_acc: 0.6400\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.6022 - acc: 0.6633 - val_loss: 0.6249 - val_acc: 0.6600\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.5975 - acc: 0.6735 - val_loss: 0.6043 - val_acc: 0.7000\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.5685 - acc: 0.6684 - val_loss: 0.6008 - val_acc: 0.6600\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 1s 391ms/step - loss: 0.5967 - acc: 0.6939 - val_loss: 0.6068 - val_acc: 0.6200\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 418ms/step - loss: 0.5822 - acc: 0.6990 - val_loss: 0.5835 - val_acc: 0.6800\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 432ms/step - loss: 0.5598 - acc: 0.6888 - val_loss: 0.5722 - val_acc: 0.7200\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 0.5445 - acc: 0.7194 - val_loss: 0.5898 - val_acc: 0.6800\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 1s 386ms/step - loss: 0.5469 - acc: 0.7143 - val_loss: 0.5776 - val_acc: 0.6800\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.5637 - acc: 0.7041 - val_loss: 0.5716 - val_acc: 0.6600\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 689ms/step - loss: 0.5599 - acc: 0.6735 - val_loss: 0.5662 - val_acc: 0.6800\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 3s 740ms/step - loss: 0.5310 - acc: 0.6990 - val_loss: 0.5804 - val_acc: 0.6800\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 505ms/step - loss: 0.5339 - acc: 0.7092 - val_loss: 0.5811 - val_acc: 0.7000\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.5347 - acc: 0.7194 - val_loss: 0.5672 - val_acc: 0.7000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.5587 - acc: 0.7143 - val_loss: 0.5768 - val_acc: 0.6600\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 503ms/step - loss: 0.5382 - acc: 0.7143 - val_loss: 0.5449 - val_acc: 0.6800\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 0.5176 - acc: 0.7347 - val_loss: 0.5699 - val_acc: 0.7000\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 398ms/step - loss: 0.5143 - acc: 0.7194 - val_loss: 0.5857 - val_acc: 0.6600\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 1s 397ms/step - loss: 0.5104 - acc: 0.7296 - val_loss: 0.5683 - val_acc: 0.7200\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.5137 - acc: 0.7347 - val_loss: 0.5433 - val_acc: 0.7200\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.5099 - acc: 0.6888 - val_loss: 0.5602 - val_acc: 0.7000\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 417ms/step - loss: 0.4922 - acc: 0.7449 - val_loss: 0.5599 - val_acc: 0.7000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.4815 - acc: 0.7602 - val_loss: 0.5545 - val_acc: 0.7200\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.4848 - acc: 0.7602 - val_loss: 0.5521 - val_acc: 0.7200\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 2s 423ms/step - loss: 0.4669 - acc: 0.7908 - val_loss: 0.5768 - val_acc: 0.6800\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.4612 - acc: 0.7806 - val_loss: 0.5788 - val_acc: 0.6800\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.4450 - acc: 0.7806 - val_loss: 0.5908 - val_acc: 0.7000\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 2s 444ms/step - loss: 0.4483 - acc: 0.7959 - val_loss: 0.6222 - val_acc: 0.6800\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 2s 424ms/step - loss: 0.4753 - acc: 0.7653 - val_loss: 0.5965 - val_acc: 0.7200\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.4611 - acc: 0.8163 - val_loss: 0.5702 - val_acc: 0.7600\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.4676 - acc: 0.7806 - val_loss: 0.5644 - val_acc: 0.7400\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.4408 - acc: 0.7959 - val_loss: 0.5841 - val_acc: 0.7000\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 2s 538ms/step - loss: 0.4350 - acc: 0.8010 - val_loss: 0.5794 - val_acc: 0.7000\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.4278 - acc: 0.7959 - val_loss: 0.6092 - val_acc: 0.7400\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.4247 - acc: 0.7857 - val_loss: 0.5789 - val_acc: 0.7600\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 2s 414ms/step - loss: 0.4201 - acc: 0.8061 - val_loss: 0.6119 - val_acc: 0.7200\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.4145 - acc: 0.8010 - val_loss: 0.6162 - val_acc: 0.7200\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 3s 765ms/step - loss: 0.5958 - acc: 0.6786 - val_loss: 0.6775 - val_acc: 0.6400\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 3s 691ms/step - loss: 0.5313 - acc: 0.7143 - val_loss: 0.6025 - val_acc: 0.7400\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.4716 - acc: 0.7755 - val_loss: 0.5621 - val_acc: 0.7400\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 2s 505ms/step - loss: 0.4773 - acc: 0.7857 - val_loss: 0.6010 - val_acc: 0.6600\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.4558 - acc: 0.7806 - val_loss: 0.6570 - val_acc: 0.7000\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.4538 - acc: 0.7806 - val_loss: 0.6780 - val_acc: 0.6600\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.4399 - acc: 0.7908 - val_loss: 0.6264 - val_acc: 0.7000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 2s 505ms/step - loss: 0.4239 - acc: 0.7806 - val_loss: 0.6465 - val_acc: 0.6800\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.4136 - acc: 0.8061 - val_loss: 0.6615 - val_acc: 0.7200\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.4110 - acc: 0.8316 - val_loss: 0.6771 - val_acc: 0.7000\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.4311 - acc: 0.7806 - val_loss: 0.6387 - val_acc: 0.7400\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.4065 - acc: 0.8061 - val_loss: 0.7067 - val_acc: 0.7400\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.3961 - acc: 0.8316 - val_loss: 0.6934 - val_acc: 0.7200\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 2s 417ms/step - loss: 0.3978 - acc: 0.8163 - val_loss: 0.7384 - val_acc: 0.7200\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 0.4282 - acc: 0.8061 - val_loss: 0.7733 - val_acc: 0.7000\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 2s 518ms/step - loss: 0.4792 - acc: 0.7449 - val_loss: 0.7428 - val_acc: 0.6400\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 2s 447ms/step - loss: 0.4077 - acc: 0.8061 - val_loss: 0.7551 - val_acc: 0.6400\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 2s 536ms/step - loss: 0.4025 - acc: 0.8061 - val_loss: 0.6925 - val_acc: 0.7400\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.4161 - acc: 0.7908 - val_loss: 0.6540 - val_acc: 0.7000\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.4550 - acc: 0.7806 - val_loss: 0.6917 - val_acc: 0.7000\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 0.4261 - acc: 0.7755 - val_loss: 0.6771 - val_acc: 0.7400\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 2s 410ms/step - loss: 0.3838 - acc: 0.8061 - val_loss: 0.7346 - val_acc: 0.7000\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 2s 414ms/step - loss: 0.3615 - acc: 0.8316 - val_loss: 0.7120 - val_acc: 0.7200\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.5702 - acc: 0.7600\n",
            "50/50 [==============================] - 2s 8ms/step\n",
            "73\n",
            "Metrics:\n",
            "Accuracy:  0.76\n",
            "f1 score:  0.7272727272727272\n",
            "precision:  0.8421052631578947\n",
            "recall:  0.64\n",
            "Specificity 0.88\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 6s 664ms/step - loss: 0.6976 - acc: 0.5000 - val_loss: 0.6912 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.6935 - acc: 0.4898 - val_loss: 0.6903 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6887 - acc: 0.5255 - val_loss: 0.6887 - val_acc: 0.5200\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6965 - acc: 0.4592 - val_loss: 0.6870 - val_acc: 0.6000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 411ms/step - loss: 0.6806 - acc: 0.6684 - val_loss: 0.6870 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 512ms/step - loss: 0.6895 - acc: 0.4745 - val_loss: 0.6843 - val_acc: 0.7400\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.6789 - acc: 0.5867 - val_loss: 0.6829 - val_acc: 0.5600\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 501ms/step - loss: 0.6822 - acc: 0.5510 - val_loss: 0.6796 - val_acc: 0.6000\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6906 - acc: 0.5459 - val_loss: 0.6776 - val_acc: 0.6400\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 451ms/step - loss: 0.6816 - acc: 0.5561 - val_loss: 0.6719 - val_acc: 0.7000\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.6771 - acc: 0.5969 - val_loss: 0.6715 - val_acc: 0.7000\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 422ms/step - loss: 0.6785 - acc: 0.6020 - val_loss: 0.6640 - val_acc: 0.7200\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 418ms/step - loss: 0.6726 - acc: 0.6378 - val_loss: 0.6586 - val_acc: 0.7000\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.6620 - acc: 0.6633 - val_loss: 0.6570 - val_acc: 0.6800\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 476ms/step - loss: 0.6565 - acc: 0.5969 - val_loss: 0.6449 - val_acc: 0.5600\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 443ms/step - loss: 0.6474 - acc: 0.6173 - val_loss: 0.6262 - val_acc: 0.7200\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 540ms/step - loss: 0.6568 - acc: 0.5612 - val_loss: 0.6280 - val_acc: 0.7600\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.6501 - acc: 0.6480 - val_loss: 0.6225 - val_acc: 0.6600\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 509ms/step - loss: 0.6439 - acc: 0.6480 - val_loss: 0.6232 - val_acc: 0.7000\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6495 - acc: 0.6071 - val_loss: 0.6406 - val_acc: 0.5600\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.6525 - acc: 0.6582 - val_loss: 0.6020 - val_acc: 0.6600\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.6402 - acc: 0.6276 - val_loss: 0.5690 - val_acc: 0.8000\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 521ms/step - loss: 0.6266 - acc: 0.6378 - val_loss: 0.5742 - val_acc: 0.7200\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 553ms/step - loss: 0.6283 - acc: 0.6786 - val_loss: 0.6149 - val_acc: 0.6000\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 3s 761ms/step - loss: 0.6303 - acc: 0.6224 - val_loss: 0.6386 - val_acc: 0.5800\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 2s 513ms/step - loss: 0.6495 - acc: 0.6429 - val_loss: 0.5822 - val_acc: 0.6800\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 515ms/step - loss: 0.5854 - acc: 0.6684 - val_loss: 0.5479 - val_acc: 0.6800\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 434ms/step - loss: 0.5687 - acc: 0.7041 - val_loss: 0.5476 - val_acc: 0.8000\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.5586 - acc: 0.7041 - val_loss: 0.5343 - val_acc: 0.7400\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.6046 - acc: 0.6837 - val_loss: 0.5413 - val_acc: 0.7200\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.5927 - acc: 0.6786 - val_loss: 0.6043 - val_acc: 0.6400\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.5661 - acc: 0.7245 - val_loss: 0.5424 - val_acc: 0.6800\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 427ms/step - loss: 0.5502 - acc: 0.7398 - val_loss: 0.5334 - val_acc: 0.7400\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.5418 - acc: 0.7551 - val_loss: 0.5255 - val_acc: 0.7800\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 552ms/step - loss: 0.5457 - acc: 0.7143 - val_loss: 0.5288 - val_acc: 0.7800\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 548ms/step - loss: 0.5366 - acc: 0.7500 - val_loss: 0.4941 - val_acc: 0.8200\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 448ms/step - loss: 0.5642 - acc: 0.7194 - val_loss: 0.5425 - val_acc: 0.8000\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.5979 - acc: 0.6582 - val_loss: 0.5613 - val_acc: 0.7000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 437ms/step - loss: 0.5573 - acc: 0.6990 - val_loss: 0.6350 - val_acc: 0.6200\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 559ms/step - loss: 0.5282 - acc: 0.7296 - val_loss: 0.4944 - val_acc: 0.8000\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 443ms/step - loss: 0.5629 - acc: 0.7194 - val_loss: 0.5154 - val_acc: 0.7800\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.5285 - acc: 0.7602 - val_loss: 0.4861 - val_acc: 0.8000\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.5236 - acc: 0.7806 - val_loss: 0.5717 - val_acc: 0.6800\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.5502 - acc: 0.7398 - val_loss: 0.5462 - val_acc: 0.6600\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.6226 - acc: 0.6378 - val_loss: 0.5672 - val_acc: 0.6600\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 515ms/step - loss: 0.6006 - acc: 0.6837 - val_loss: 0.5534 - val_acc: 0.6400\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.6336 - acc: 0.6173 - val_loss: 0.5508 - val_acc: 0.7400\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 503ms/step - loss: 0.5592 - acc: 0.6939 - val_loss: 0.5443 - val_acc: 0.7000\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 2s 406ms/step - loss: 0.6092 - acc: 0.7143 - val_loss: 0.5207 - val_acc: 0.7600\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.6486 - acc: 0.5969 - val_loss: 0.5578 - val_acc: 0.7600\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 3s 741ms/step - loss: 0.5716 - acc: 0.6378 - val_loss: 0.6153 - val_acc: 0.6400\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.5279 - acc: 0.7398 - val_loss: 0.5615 - val_acc: 0.7600\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 2s 504ms/step - loss: 0.5783 - acc: 0.6327 - val_loss: 0.5549 - val_acc: 0.8000\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.5253 - acc: 0.7347 - val_loss: 0.5644 - val_acc: 0.6400\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.5209 - acc: 0.7194 - val_loss: 0.5456 - val_acc: 0.7400\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.4946 - acc: 0.7653 - val_loss: 0.5571 - val_acc: 0.7400\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.5297 - acc: 0.7500 - val_loss: 0.5374 - val_acc: 0.7800\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.4943 - acc: 0.7755 - val_loss: 0.5361 - val_acc: 0.7400\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.4910 - acc: 0.7653 - val_loss: 0.5228 - val_acc: 0.7800\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.4913 - acc: 0.7704 - val_loss: 0.5151 - val_acc: 0.7400\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 2s 393ms/step - loss: 0.4801 - acc: 0.8061 - val_loss: 0.5409 - val_acc: 0.7800\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.4905 - acc: 0.7398 - val_loss: 0.5052 - val_acc: 0.8000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 2s 420ms/step - loss: 0.4731 - acc: 0.7653 - val_loss: 0.5101 - val_acc: 0.8000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.4564 - acc: 0.7806 - val_loss: 0.5036 - val_acc: 0.7800\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 2s 430ms/step - loss: 0.4560 - acc: 0.7806 - val_loss: 0.4980 - val_acc: 0.7600\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 2s 537ms/step - loss: 0.4676 - acc: 0.8010 - val_loss: 0.4983 - val_acc: 0.7800\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.4941 - acc: 0.8200\n",
            "50/50 [==============================] - 1s 9ms/step\n",
            "40\n",
            "Metrics:\n",
            "Accuracy:  0.82\n",
            "f1 score:  0.7999999999999999\n",
            "precision:  0.9\n",
            "recall:  0.72\n",
            "Specificity 0.92\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 6s 698ms/step - loss: 0.6955 - acc: 0.4745 - val_loss: 0.6940 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 432ms/step - loss: 0.6936 - acc: 0.5000 - val_loss: 0.6916 - val_acc: 0.6400\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 449ms/step - loss: 0.6913 - acc: 0.5459 - val_loss: 0.6924 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 554ms/step - loss: 0.6977 - acc: 0.5000 - val_loss: 0.6924 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 478ms/step - loss: 0.6906 - acc: 0.5612 - val_loss: 0.6920 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 3s 764ms/step - loss: 0.6926 - acc: 0.5204 - val_loss: 0.6923 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 3s 686ms/step - loss: 0.6964 - acc: 0.4796 - val_loss: 0.6900 - val_acc: 0.5800\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 0.6904 - acc: 0.5357 - val_loss: 0.6894 - val_acc: 0.5800\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 536ms/step - loss: 0.6912 - acc: 0.5102 - val_loss: 0.6890 - val_acc: 0.6400\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 448ms/step - loss: 0.6875 - acc: 0.5357 - val_loss: 0.6879 - val_acc: 0.4800\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 546ms/step - loss: 0.6894 - acc: 0.5357 - val_loss: 0.6848 - val_acc: 0.6000\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.6837 - acc: 0.5867 - val_loss: 0.6817 - val_acc: 0.6200\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 419ms/step - loss: 0.6771 - acc: 0.5969 - val_loss: 0.6778 - val_acc: 0.6400\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 437ms/step - loss: 0.6784 - acc: 0.6122 - val_loss: 0.6716 - val_acc: 0.6600\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.6735 - acc: 0.6122 - val_loss: 0.6655 - val_acc: 0.6400\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 439ms/step - loss: 0.6733 - acc: 0.5714 - val_loss: 0.6616 - val_acc: 0.5400\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 420ms/step - loss: 0.6673 - acc: 0.6224 - val_loss: 0.6461 - val_acc: 0.6400\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 511ms/step - loss: 0.6435 - acc: 0.6582 - val_loss: 0.6305 - val_acc: 0.6600\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6471 - acc: 0.6020 - val_loss: 0.6162 - val_acc: 0.6800\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.6281 - acc: 0.6429 - val_loss: 0.6169 - val_acc: 0.6600\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.6285 - acc: 0.6582 - val_loss: 0.6018 - val_acc: 0.6600\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 426ms/step - loss: 0.6141 - acc: 0.6480 - val_loss: 0.5845 - val_acc: 0.7600\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.5924 - acc: 0.7041 - val_loss: 0.5838 - val_acc: 0.7000\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.6071 - acc: 0.6582 - val_loss: 0.5913 - val_acc: 0.7200\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 416ms/step - loss: 0.6042 - acc: 0.6735 - val_loss: 0.6019 - val_acc: 0.7400\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.5983 - acc: 0.6633 - val_loss: 0.5987 - val_acc: 0.6600\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.6198 - acc: 0.6684 - val_loss: 0.5981 - val_acc: 0.7200\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.5864 - acc: 0.6786 - val_loss: 0.6027 - val_acc: 0.7200\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.5744 - acc: 0.7092 - val_loss: 0.6068 - val_acc: 0.6800\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 422ms/step - loss: 0.5841 - acc: 0.6990 - val_loss: 0.6198 - val_acc: 0.6600\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.5879 - acc: 0.6990 - val_loss: 0.6073 - val_acc: 0.6800\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 397ms/step - loss: 0.5974 - acc: 0.6990 - val_loss: 0.6135 - val_acc: 0.6600\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.5831 - acc: 0.6582 - val_loss: 0.6081 - val_acc: 0.7200\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 505ms/step - loss: 0.5820 - acc: 0.6888 - val_loss: 0.5958 - val_acc: 0.6800\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 3s 762ms/step - loss: 0.5785 - acc: 0.6786 - val_loss: 0.6082 - val_acc: 0.7000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.5899 - acc: 0.6684 - val_loss: 0.6041 - val_acc: 0.7000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.5902 - acc: 0.6684 - val_loss: 0.6091 - val_acc: 0.6400\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 513ms/step - loss: 0.5829 - acc: 0.6786 - val_loss: 0.6203 - val_acc: 0.7000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 418ms/step - loss: 0.5842 - acc: 0.6990 - val_loss: 0.6298 - val_acc: 0.6200\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.5767 - acc: 0.7041 - val_loss: 0.6238 - val_acc: 0.6600\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 406ms/step - loss: 0.6017 - acc: 0.6633 - val_loss: 0.6225 - val_acc: 0.7000\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 504ms/step - loss: 0.5833 - acc: 0.6888 - val_loss: 0.5962 - val_acc: 0.7400\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.5633 - acc: 0.6888 - val_loss: 0.5797 - val_acc: 0.7400\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 551ms/step - loss: 0.5594 - acc: 0.6888 - val_loss: 0.5627 - val_acc: 0.7400\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 2s 434ms/step - loss: 0.5515 - acc: 0.6939 - val_loss: 0.5568 - val_acc: 0.7800\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 405ms/step - loss: 0.5448 - acc: 0.7194 - val_loss: 0.5544 - val_acc: 0.7000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.5384 - acc: 0.7092 - val_loss: 0.6005 - val_acc: 0.7400\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 532ms/step - loss: 0.5542 - acc: 0.7245 - val_loss: 0.5617 - val_acc: 0.7200\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.5439 - acc: 0.7143 - val_loss: 0.5537 - val_acc: 0.7200\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 536ms/step - loss: 0.5440 - acc: 0.7245 - val_loss: 0.6039 - val_acc: 0.6800\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.5609 - acc: 0.6786 - val_loss: 0.5373 - val_acc: 0.7200\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.5254 - acc: 0.7194 - val_loss: 0.5433 - val_acc: 0.7800\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.5253 - acc: 0.7143 - val_loss: 0.5295 - val_acc: 0.7600\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 2s 574ms/step - loss: 0.5285 - acc: 0.7194 - val_loss: 0.6431 - val_acc: 0.6800\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 2s 595ms/step - loss: 0.5340 - acc: 0.7092 - val_loss: 0.5769 - val_acc: 0.7200\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 3s 726ms/step - loss: 0.5130 - acc: 0.7092 - val_loss: 0.5290 - val_acc: 0.6800\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 2s 426ms/step - loss: 0.4980 - acc: 0.7296 - val_loss: 0.5326 - val_acc: 0.7400\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.5017 - acc: 0.7347 - val_loss: 0.5546 - val_acc: 0.8000\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 2s 521ms/step - loss: 0.5087 - acc: 0.7398 - val_loss: 0.5582 - val_acc: 0.7000\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 2s 671ms/step - loss: 0.4900 - acc: 0.7500 - val_loss: 0.5596 - val_acc: 0.7000\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 3s 705ms/step - loss: 0.4836 - acc: 0.7857 - val_loss: 0.5568 - val_acc: 0.7600\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 2s 402ms/step - loss: 0.5145 - acc: 0.7296 - val_loss: 0.6615 - val_acc: 0.6800\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 2s 416ms/step - loss: 0.5364 - acc: 0.7245 - val_loss: 0.6571 - val_acc: 0.6600\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.5407 - acc: 0.7194 - val_loss: 0.5859 - val_acc: 0.7200\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.5157 - acc: 0.7347 - val_loss: 0.5740 - val_acc: 0.6400\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.4796 - acc: 0.7602 - val_loss: 0.5414 - val_acc: 0.7800\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 2s 413ms/step - loss: 0.5110 - acc: 0.7041 - val_loss: 0.5714 - val_acc: 0.6600\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 2s 509ms/step - loss: 0.5101 - acc: 0.7194 - val_loss: 0.5649 - val_acc: 0.6800\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 2s 406ms/step - loss: 0.5235 - acc: 0.7500 - val_loss: 0.5681 - val_acc: 0.7800\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.4751 - acc: 0.7449 - val_loss: 0.5740 - val_acc: 0.6600\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 2s 403ms/step - loss: 0.4704 - acc: 0.7602 - val_loss: 0.5679 - val_acc: 0.7600\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.4651 - acc: 0.7704 - val_loss: 0.6297 - val_acc: 0.6800\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.4530 - acc: 0.7908 - val_loss: 0.6043 - val_acc: 0.7600\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 2s 407ms/step - loss: 0.4589 - acc: 0.7500 - val_loss: 0.5922 - val_acc: 0.6800\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.4360 - acc: 0.8061 - val_loss: 0.5503 - val_acc: 0.7200\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.4393 - acc: 0.7551 - val_loss: 0.6037 - val_acc: 0.6600\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 2s 399ms/step - loss: 0.4589 - acc: 0.7653 - val_loss: 0.5537 - val_acc: 0.6800\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.4327 - acc: 0.7755 - val_loss: 0.5981 - val_acc: 0.7200\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 2s 433ms/step - loss: 0.4703 - acc: 0.7296 - val_loss: 0.6578 - val_acc: 0.6400\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.4747 - acc: 0.7143 - val_loss: 0.6210 - val_acc: 0.7400\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 2s 543ms/step - loss: 0.4747 - acc: 0.7143 - val_loss: 0.5995 - val_acc: 0.6600\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 2s 453ms/step - loss: 0.4684 - acc: 0.7704 - val_loss: 0.6015 - val_acc: 0.7600\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 2s 554ms/step - loss: 0.4547 - acc: 0.7551 - val_loss: 0.5615 - val_acc: 0.6400\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.4494 - acc: 0.7653 - val_loss: 0.5633 - val_acc: 0.7400\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 2s 543ms/step - loss: 0.4295 - acc: 0.8061 - val_loss: 0.5550 - val_acc: 0.6800\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 3s 849ms/step - loss: 0.4027 - acc: 0.7959 - val_loss: 0.5617 - val_acc: 0.7400\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 2s 446ms/step - loss: 0.4123 - acc: 0.7908 - val_loss: 0.5740 - val_acc: 0.7200\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 2s 455ms/step - loss: 0.4003 - acc: 0.7959 - val_loss: 0.6026 - val_acc: 0.6800\n",
            "50/50 [==============================] - 1s 9ms/step - loss: 0.5546 - acc: 0.8000\n",
            "50/50 [==============================] - 1s 9ms/step\n",
            "45\n",
            "Metrics:\n",
            "Accuracy:  0.8\n",
            "f1 score:  0.7727272727272727\n",
            "precision:  0.8947368421052632\n",
            "recall:  0.68\n",
            "Specificity 0.92\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 6s 712ms/step - loss: 0.7010 - acc: 0.4643 - val_loss: 0.6914 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.6909 - acc: 0.5510 - val_loss: 0.6912 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 464ms/step - loss: 0.6868 - acc: 0.5510 - val_loss: 0.6900 - val_acc: 0.5200\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 532ms/step - loss: 0.6786 - acc: 0.6071 - val_loss: 0.6882 - val_acc: 0.5800\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 439ms/step - loss: 0.6929 - acc: 0.5306 - val_loss: 0.6878 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 449ms/step - loss: 0.6889 - acc: 0.5306 - val_loss: 0.6873 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.6884 - acc: 0.5357 - val_loss: 0.6870 - val_acc: 0.5200\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.6885 - acc: 0.5357 - val_loss: 0.6814 - val_acc: 0.6000\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 535ms/step - loss: 0.6941 - acc: 0.5255 - val_loss: 0.6786 - val_acc: 0.6000\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.6799 - acc: 0.5510 - val_loss: 0.6766 - val_acc: 0.6200\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 536ms/step - loss: 0.6664 - acc: 0.6020 - val_loss: 0.6680 - val_acc: 0.6400\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 442ms/step - loss: 0.6680 - acc: 0.5663 - val_loss: 0.6604 - val_acc: 0.6200\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.6708 - acc: 0.5663 - val_loss: 0.6559 - val_acc: 0.6400\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 444ms/step - loss: 0.6451 - acc: 0.6378 - val_loss: 0.6503 - val_acc: 0.6200\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 472ms/step - loss: 0.6351 - acc: 0.6327 - val_loss: 0.6467 - val_acc: 0.6200\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 552ms/step - loss: 0.6383 - acc: 0.6480 - val_loss: 0.6282 - val_acc: 0.6800\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 547ms/step - loss: 0.5987 - acc: 0.7041 - val_loss: 0.6217 - val_acc: 0.6000\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 535ms/step - loss: 0.5918 - acc: 0.6633 - val_loss: 0.6095 - val_acc: 0.6400\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 658ms/step - loss: 0.6128 - acc: 0.6990 - val_loss: 0.5950 - val_acc: 0.6200\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 3s 838ms/step - loss: 0.6277 - acc: 0.6480 - val_loss: 0.6246 - val_acc: 0.6800\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 554ms/step - loss: 0.6109 - acc: 0.6633 - val_loss: 0.5973 - val_acc: 0.6400\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 468ms/step - loss: 0.6075 - acc: 0.6378 - val_loss: 0.5913 - val_acc: 0.6800\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 449ms/step - loss: 0.6138 - acc: 0.6633 - val_loss: 0.6123 - val_acc: 0.6800\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 538ms/step - loss: 0.5930 - acc: 0.6990 - val_loss: 0.6353 - val_acc: 0.6400\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 442ms/step - loss: 0.5913 - acc: 0.7041 - val_loss: 0.5964 - val_acc: 0.6200\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 2s 555ms/step - loss: 0.5713 - acc: 0.7041 - val_loss: 0.5914 - val_acc: 0.6800\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 447ms/step - loss: 0.5622 - acc: 0.7194 - val_loss: 0.5870 - val_acc: 0.6600\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.5522 - acc: 0.7296 - val_loss: 0.5900 - val_acc: 0.6800\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.5527 - acc: 0.7245 - val_loss: 0.5910 - val_acc: 0.6600\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 537ms/step - loss: 0.5500 - acc: 0.7398 - val_loss: 0.5908 - val_acc: 0.6400\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 557ms/step - loss: 0.5501 - acc: 0.7296 - val_loss: 0.5862 - val_acc: 0.6800\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 444ms/step - loss: 0.5520 - acc: 0.7143 - val_loss: 0.5902 - val_acc: 0.6800\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 553ms/step - loss: 0.5411 - acc: 0.7449 - val_loss: 0.5905 - val_acc: 0.6600\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 535ms/step - loss: 0.5400 - acc: 0.7194 - val_loss: 0.5954 - val_acc: 0.6800\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 447ms/step - loss: 0.5343 - acc: 0.7347 - val_loss: 0.5976 - val_acc: 0.6400\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.5349 - acc: 0.7245 - val_loss: 0.6219 - val_acc: 0.6600\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.5338 - acc: 0.7194 - val_loss: 0.6072 - val_acc: 0.6800\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 444ms/step - loss: 0.5326 - acc: 0.7602 - val_loss: 0.6036 - val_acc: 0.6600\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.5258 - acc: 0.7245 - val_loss: 0.6294 - val_acc: 0.5800\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.5212 - acc: 0.7500 - val_loss: 0.6247 - val_acc: 0.6400\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 578ms/step - loss: 0.5191 - acc: 0.7449 - val_loss: 0.6246 - val_acc: 0.6400\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 448ms/step - loss: 0.5231 - acc: 0.7398 - val_loss: 0.6555 - val_acc: 0.6800\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 451ms/step - loss: 0.5204 - acc: 0.7449 - val_loss: 0.6322 - val_acc: 0.6400\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 625ms/step - loss: 0.4988 - acc: 0.7551 - val_loss: 0.6300 - val_acc: 0.6600\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 3s 813ms/step - loss: 0.4932 - acc: 0.7347 - val_loss: 0.6663 - val_acc: 0.6400\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 462ms/step - loss: 0.5014 - acc: 0.7347 - val_loss: 0.6421 - val_acc: 0.6400\n",
            "50/50 [==============================] - 1s 10ms/step - loss: 0.6282 - acc: 0.6800\n",
            "50/50 [==============================] - 1s 9ms/step\n",
            "75\n",
            "Metrics:\n",
            "Accuracy:  0.68\n",
            "f1 score:  0.7241379310344828\n",
            "precision:  0.6363636363636364\n",
            "recall:  0.84\n",
            "Specificity 0.52\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 6s 778ms/step - loss: 0.6964 - acc: 0.5000 - val_loss: 0.6926 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 455ms/step - loss: 0.7012 - acc: 0.4592 - val_loss: 0.6915 - val_acc: 0.5400\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.6914 - acc: 0.5510 - val_loss: 0.6916 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 447ms/step - loss: 0.6955 - acc: 0.4847 - val_loss: 0.6905 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 440ms/step - loss: 0.6905 - acc: 0.5510 - val_loss: 0.6903 - val_acc: 0.7000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 2s 466ms/step - loss: 0.6888 - acc: 0.5561 - val_loss: 0.6897 - val_acc: 0.5400\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 454ms/step - loss: 0.6862 - acc: 0.5561 - val_loss: 0.6889 - val_acc: 0.7000\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 440ms/step - loss: 0.6921 - acc: 0.4949 - val_loss: 0.6882 - val_acc: 0.6200\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 449ms/step - loss: 0.6905 - acc: 0.5051 - val_loss: 0.6868 - val_acc: 0.6600\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 578ms/step - loss: 0.6887 - acc: 0.5204 - val_loss: 0.6853 - val_acc: 0.7200\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 559ms/step - loss: 0.6808 - acc: 0.6071 - val_loss: 0.6840 - val_acc: 0.6400\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 532ms/step - loss: 0.6791 - acc: 0.6327 - val_loss: 0.6820 - val_acc: 0.6000\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 435ms/step - loss: 0.6793 - acc: 0.5867 - val_loss: 0.6795 - val_acc: 0.6000\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 539ms/step - loss: 0.6744 - acc: 0.6071 - val_loss: 0.6771 - val_acc: 0.5800\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 535ms/step - loss: 0.6743 - acc: 0.6173 - val_loss: 0.6757 - val_acc: 0.6400\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 481ms/step - loss: 0.6557 - acc: 0.6735 - val_loss: 0.6656 - val_acc: 0.6000\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 549ms/step - loss: 0.6549 - acc: 0.6684 - val_loss: 0.6564 - val_acc: 0.6200\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 545ms/step - loss: 0.6463 - acc: 0.6684 - val_loss: 0.6571 - val_acc: 0.6000\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 571ms/step - loss: 0.6437 - acc: 0.6224 - val_loss: 0.6474 - val_acc: 0.6200\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 3s 829ms/step - loss: 0.6413 - acc: 0.6173 - val_loss: 0.6471 - val_acc: 0.6600\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 560ms/step - loss: 0.6421 - acc: 0.6071 - val_loss: 0.6382 - val_acc: 0.6400\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 552ms/step - loss: 0.6527 - acc: 0.6327 - val_loss: 0.6197 - val_acc: 0.7000\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 549ms/step - loss: 0.6227 - acc: 0.6990 - val_loss: 0.6176 - val_acc: 0.7000\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 464ms/step - loss: 0.6140 - acc: 0.6939 - val_loss: 0.6059 - val_acc: 0.6800\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 467ms/step - loss: 0.6018 - acc: 0.6684 - val_loss: 0.5770 - val_acc: 0.6800\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 2s 540ms/step - loss: 0.5818 - acc: 0.6990 - val_loss: 0.5849 - val_acc: 0.7200\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 466ms/step - loss: 0.5839 - acc: 0.6837 - val_loss: 0.5776 - val_acc: 0.6800\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 551ms/step - loss: 0.5883 - acc: 0.6684 - val_loss: 0.5542 - val_acc: 0.6600\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 589ms/step - loss: 0.5416 - acc: 0.7143 - val_loss: 0.5499 - val_acc: 0.6600\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 572ms/step - loss: 0.5486 - acc: 0.7245 - val_loss: 0.5279 - val_acc: 0.7000\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 569ms/step - loss: 0.5437 - acc: 0.6888 - val_loss: 0.5510 - val_acc: 0.7200\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 565ms/step - loss: 0.5315 - acc: 0.7143 - val_loss: 0.5378 - val_acc: 0.7400\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 485ms/step - loss: 0.5276 - acc: 0.7143 - val_loss: 0.5411 - val_acc: 0.7200\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 461ms/step - loss: 0.5168 - acc: 0.7245 - val_loss: 0.5148 - val_acc: 0.7400\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 465ms/step - loss: 0.5260 - acc: 0.7092 - val_loss: 0.5113 - val_acc: 0.7000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 465ms/step - loss: 0.5208 - acc: 0.7194 - val_loss: 0.5201 - val_acc: 0.7000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 468ms/step - loss: 0.5012 - acc: 0.7347 - val_loss: 0.4996 - val_acc: 0.7200\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.4955 - acc: 0.7653 - val_loss: 0.4956 - val_acc: 0.7200\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 463ms/step - loss: 0.4886 - acc: 0.7398 - val_loss: 0.5065 - val_acc: 0.6800\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 543ms/step - loss: 0.4855 - acc: 0.7500 - val_loss: 0.4978 - val_acc: 0.7200\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 430ms/step - loss: 0.4925 - acc: 0.7398 - val_loss: 0.5144 - val_acc: 0.7400\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 546ms/step - loss: 0.4731 - acc: 0.7551 - val_loss: 0.5208 - val_acc: 0.7200\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 546ms/step - loss: 0.4546 - acc: 0.7704 - val_loss: 0.5044 - val_acc: 0.7200\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 518ms/step - loss: 0.4632 - acc: 0.7704 - val_loss: 0.5356 - val_acc: 0.7000\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 3s 833ms/step - loss: 0.4499 - acc: 0.7755 - val_loss: 0.5003 - val_acc: 0.7400\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 480ms/step - loss: 0.4268 - acc: 0.7959 - val_loss: 0.5579 - val_acc: 0.7600\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 473ms/step - loss: 0.4496 - acc: 0.7959 - val_loss: 0.4875 - val_acc: 0.7200\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 548ms/step - loss: 0.4136 - acc: 0.8163 - val_loss: 0.5726 - val_acc: 0.7400\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 2s 536ms/step - loss: 0.4527 - acc: 0.7806 - val_loss: 0.5072 - val_acc: 0.7400\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 548ms/step - loss: 0.3872 - acc: 0.8622 - val_loss: 0.5681 - val_acc: 0.7400\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.4024 - acc: 0.8163 - val_loss: 0.5083 - val_acc: 0.6800\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 2s 442ms/step - loss: 0.4333 - acc: 0.7857 - val_loss: 0.6208 - val_acc: 0.7200\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 2s 545ms/step - loss: 0.4151 - acc: 0.8163 - val_loss: 0.5098 - val_acc: 0.6600\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 2s 448ms/step - loss: 0.4133 - acc: 0.7959 - val_loss: 0.6166 - val_acc: 0.7200\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 2s 430ms/step - loss: 0.4351 - acc: 0.7704 - val_loss: 0.5113 - val_acc: 0.6600\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 2s 554ms/step - loss: 0.4515 - acc: 0.7908 - val_loss: 0.5429 - val_acc: 0.7400\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 2s 579ms/step - loss: 0.4513 - acc: 0.7347 - val_loss: 0.5317 - val_acc: 0.7000\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 2s 558ms/step - loss: 0.4161 - acc: 0.7959 - val_loss: 0.5525 - val_acc: 0.6600\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 2s 487ms/step - loss: 0.3916 - acc: 0.8316 - val_loss: 0.6076 - val_acc: 0.7400\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 2s 561ms/step - loss: 0.3982 - acc: 0.8265 - val_loss: 0.5148 - val_acc: 0.7400\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 2s 451ms/step - loss: 0.3826 - acc: 0.8214 - val_loss: 0.5962 - val_acc: 0.6800\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 2s 579ms/step - loss: 0.3816 - acc: 0.8061 - val_loss: 0.5297 - val_acc: 0.7200\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 2s 483ms/step - loss: 0.4066 - acc: 0.7806 - val_loss: 0.6411 - val_acc: 0.7000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 2s 562ms/step - loss: 0.4016 - acc: 0.7755 - val_loss: 0.5548 - val_acc: 0.7000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 2s 484ms/step - loss: 0.3571 - acc: 0.8622 - val_loss: 0.5453 - val_acc: 0.6800\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 2s 454ms/step - loss: 0.3635 - acc: 0.8112 - val_loss: 0.5903 - val_acc: 0.6600\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 2s 489ms/step - loss: 0.3649 - acc: 0.8214 - val_loss: 0.5747 - val_acc: 0.7000\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 2s 497ms/step - loss: 0.3365 - acc: 0.8418 - val_loss: 0.5577 - val_acc: 0.7200\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 3s 818ms/step - loss: 0.3369 - acc: 0.8367 - val_loss: 0.5716 - val_acc: 0.7000\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 3s 711ms/step - loss: 0.3393 - acc: 0.8418 - val_loss: 0.5372 - val_acc: 0.7400\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 2s 509ms/step - loss: 0.3705 - acc: 0.8265 - val_loss: 0.6680 - val_acc: 0.7200\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 2s 584ms/step - loss: 0.3457 - acc: 0.8418 - val_loss: 0.5583 - val_acc: 0.6800\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.3295 - acc: 0.8367 - val_loss: 0.5660 - val_acc: 0.7200\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 2s 599ms/step - loss: 0.3041 - acc: 0.8622 - val_loss: 0.5579 - val_acc: 0.7000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 2s 538ms/step - loss: 0.3172 - acc: 0.8571 - val_loss: 0.6157 - val_acc: 0.7200\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 2s 541ms/step - loss: 0.3125 - acc: 0.8673 - val_loss: 0.6021 - val_acc: 0.7200\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 0.5579 - acc: 0.7600\n",
            "50/50 [==============================] - 1s 11ms/step\n",
            "69\n",
            "Metrics:\n",
            "Accuracy:  0.76\n",
            "f1 score:  0.7000000000000001\n",
            "precision:  0.9333333333333333\n",
            "recall:  0.56\n",
            "Specificity 0.96\n",
            "\n",
            "\n",
            "\n",
            "Average accccuracy: 0.770 +/- 0.041\n",
            "Average precision: 0.807 +/- 0.087\n",
            "Average recall: 0.736 +/- 0.082\n",
            "Average f1: 0.761 +/- 0.037 \n",
            "\n",
            "\n",
            "Average specificity: 0.804 +/- 0.125 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stellargraph.interpretability.saliency_maps import IntegratedGradients"
      ],
      "metadata": {
        "id": "y17sWfIgBhx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best RS from 40 to 80:  \n",
        "\n",
        "62 really good rec, but others not so good \n",
        "51(p good) \n",
        "\n",
        "75, 69    \n",
        "all 80\n",
        "\n",
        "73 (one of the best)  \n",
        "acc- 0.84   \n",
        "f1- 0.86   \n",
        "prec- 0.94    \n",
        "rec- 0.76    \n",
        "spec- 0.96  \n",
        "\n",
        "55  \n",
        "acc- 0.8    \n",
        "f1- 0.82   \n",
        "prec- 0.742  \n",
        "rec- 0.92   \n",
        "spec- 0.68    \n",
        "\n",
        "40  \n",
        "acc- 0.84  \n",
        "f1- 0.8333  \n",
        "prec- 0.87  \n",
        "rec- 0.80  \n",
        "spec- 0.88  \n",
        "\n",
        "\n",
        "45  \n",
        "acc- 0.82   \n",
        "f1- 0.75  \n",
        "prec- 1.0  \n",
        "rec- 0.62  \n",
        "spec- 1.0  \n",
        "\n"
      ],
      "metadata": {
        "id": "o0e64ELiiVFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iter 1:\n",
        "\n",
        "Accuracy:  0.86 <br>\n",
        "f1 score:  0.8627450980392156<br>\n",
        "precision:  0.8461538461538461<br>\n",
        "recall:  0.88<br>\n",
        "specificity: 0.84 <br>\n",
        "\n",
        "iter 4: <br>\n",
        "Accuracy:  0.86 <br>\n",
        "f1 score:  0.8627450980392156 <br>\n",
        "precision:  0.8461538461538461 <br>\n",
        "recall:  0.88 <br>\n",
        "specificity  0.84 <br>\n",
        "\n",
        "iter 6: <br>\n",
        "Accuracy:  0.78 <br>\n",
        "f1 score:  0.7843137254901961 <br>\n",
        "precision:  0.7692307692307693 <br>\n",
        "recall:  0.8 <br>\n",
        "specificity  0.76 <br>\n",
        "\n",
        "iter 7: <br>\n",
        "Accuracy:  0.78 <br>\n",
        "f1 score:  0.8070175438596492 <br>\n",
        "precision:  0.7875 <br>\n",
        "recall:  0.92 <br>\n",
        "specificity  0.68 <br>\n",
        "\n",
        "iter 9: <br>\n",
        "Accuracy:  0.82 <br>\n",
        "f1 score:  0.7999999999999999 <br>\n",
        "precision:  0.8 <br>\n",
        "recall:  0.92 <br>\n",
        "specificity  0.79 <br>\n",
        "\n",
        "77  \n",
        "acc- 0.76    \n",
        "f1- 0.80    \n",
        "prec- 0.70   \n",
        "rec- 0.92   \n",
        "spec- 0.62    "
      ],
      "metadata": {
        "id": "yjq9E1I7TRlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.76,0.82, 0.78, 0.78, 0.86, 0.86, 0.82, 0.84, 0.80, 0.84 ]\n",
        "print(\"acc\")\n",
        "print(np.mean(acc), \"±\", np.std(acc) )\n",
        "\n",
        "f1 = [0.70, 0.80, 0.79, 0.80, 0.79, 0.86, 0.75, 0.83, 0.82, 0.86]\n",
        "print(\"f1\")\n",
        "print(np.mean(f1), \"±\",  np.std(f1))\n",
        "\n",
        "prec = [0.8, 0.78, 0.80, 0.76, 0.84, 0.85, 1.0, 0.87, 0.742, 0.94]\n",
        "print(\"prec\")\n",
        "print(np.mean(prec), \"±\", np.std(prec) )\n",
        "\n",
        "rec = [0.76, 0.92, 0.80, 0.92, 0.80, 0.88, 0.88, 0.80, 0.92, 0.92, 0.92]\n",
        "print(\"rec\")\n",
        "print(np.mean(rec), \"±\" , np.std(rec))\n",
        "\n",
        "spec = [0.80, 0.96, 0.88, 1.0 , 0.84, 0.84, 0.76, 0.72, 0.79, 0.80]\n",
        "print(\"spec\")\n",
        "print(np.mean(spec), \"±\" , np.std(spec))"
      ],
      "metadata": {
        "id": "PSjLTB8OlRIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563f9cc1-4f40-4e2a-8c8a-316c68c5ef16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc\n",
            "0.8160000000000001 ± 0.03322649545167228\n",
            "f1\n",
            "0.8 ± 0.04604345773288535\n",
            "prec\n",
            "0.8382 ± 0.07705296879420025\n",
            "rec\n",
            "0.8654545454545456 ± 0.059751551727836305\n",
            "spec\n",
            "0.8390000000000001 ± 0.08251666498350499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "iter 5: <br>\n",
        "Accuracy:  0.78  <br>\n",
        "f1 score:  0.7886206896551725  <br>\n",
        "precision:  0.698 <br>\n",
        "recall:  0.88  <br>\n",
        "specificity  0.66  <br>"
      ],
      "metadata": {
        "id": "s0Gkd6QzxBDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split brain"
      ],
      "metadata": {
        "id": "GfRkc5baiVK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3iI3RhHngaN"
      },
      "outputs": [],
      "source": [
        "rs=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N43hiVDOu-r"
      },
      "source": [
        "RS splits for augmented <br>\n",
        "1,4,5(best),10,17,18,20(best),21(best), 24, 25, 32, 34(best yet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCTOgpXZVtKO"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor=\"val_acc\", min_delta=0, patience=30, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltlt1y0tvlYI"
      },
      "outputs": [],
      "source": [
        "k = 20 \n",
        "in_feat = 20\n",
        "layer_sizes = [in_feat, 32, 64, 128, 128, 64, 32, 16, 4, 2]\n",
        "dgcnn_model = DeepGraphCNN(\n",
        "    layer_sizes=layer_sizes,\n",
        "    activations=[\"leaky_relu\", \"leaky_relu\", \"leaky_relu\",  \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"log_softmax\"],\n",
        "    k=k,\n",
        "    bias=True,\n",
        "    generator=generator,\n",
        ")\n",
        "x_inp, x_out = dgcnn_model.in_out_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL5CgySJvpp2"
      },
      "outputs": [],
      "source": [
        "x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
        "x_out = MaxPool1D(pool_size=2)(x_out)\n",
        "x_out = Conv1D(filters=32, kernel_size=4, strides=1)(x_out)\n",
        "x_out = Flatten()(x_out)\n",
        "x_out = Dense(units=128, activation=\"leaky_relu\")(x_out)\n",
        "x_out = Dropout(rate=0.5)(x_out)\n",
        "\n",
        "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPkT6Z3nv1Jv"
      },
      "outputs": [],
      "source": [
        "epochs=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBj9q5WYxXWC",
        "outputId": "a386a77a-797c-4246-af0c-f307bbd0bd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 7s 1s/step - loss: 0.6949 - acc: 0.4694 - val_loss: 0.6915 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.6887 - acc: 0.5918 - val_loss: 0.6908 - val_acc: 0.7200\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.6861 - acc: 0.5255 - val_loss: 0.6913 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 2s 434ms/step - loss: 0.6937 - acc: 0.5102 - val_loss: 0.6912 - val_acc: 0.5200\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 2s 687ms/step - loss: 0.6959 - acc: 0.5357 - val_loss: 0.6904 - val_acc: 0.6000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 3s 703ms/step - loss: 0.6963 - acc: 0.5153 - val_loss: 0.6894 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.6960 - acc: 0.4949 - val_loss: 0.6885 - val_acc: 0.5400\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 2s 422ms/step - loss: 0.6857 - acc: 0.5561 - val_loss: 0.6855 - val_acc: 0.5400\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 0.6843 - acc: 0.5663 - val_loss: 0.6827 - val_acc: 0.6200\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 2s 532ms/step - loss: 0.6723 - acc: 0.6429 - val_loss: 0.6798 - val_acc: 0.5200\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 2s 426ms/step - loss: 0.6681 - acc: 0.6378 - val_loss: 0.6755 - val_acc: 0.5200\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 2s 419ms/step - loss: 0.6643 - acc: 0.6122 - val_loss: 0.6720 - val_acc: 0.5200\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.6594 - acc: 0.6327 - val_loss: 0.6614 - val_acc: 0.5400\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.6438 - acc: 0.6735 - val_loss: 0.6498 - val_acc: 0.5200\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6280 - acc: 0.6582 - val_loss: 0.6359 - val_acc: 0.5800\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.6218 - acc: 0.6837 - val_loss: 0.6294 - val_acc: 0.6200\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 2s 433ms/step - loss: 0.5932 - acc: 0.6888 - val_loss: 0.6241 - val_acc: 0.6400\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.5790 - acc: 0.6735 - val_loss: 0.6047 - val_acc: 0.7200\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 2s 417ms/step - loss: 0.5651 - acc: 0.7041 - val_loss: 0.6544 - val_acc: 0.6400\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 2s 428ms/step - loss: 0.5661 - acc: 0.7041 - val_loss: 0.6213 - val_acc: 0.6600\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.5720 - acc: 0.6837 - val_loss: 0.6211 - val_acc: 0.6400\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 2s 418ms/step - loss: 0.5560 - acc: 0.7092 - val_loss: 0.6411 - val_acc: 0.6600\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.5852 - acc: 0.6582 - val_loss: 0.6188 - val_acc: 0.6200\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.5494 - acc: 0.6888 - val_loss: 0.6243 - val_acc: 0.6600\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.5714 - acc: 0.6786 - val_loss: 0.6188 - val_acc: 0.6400\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 3s 799ms/step - loss: 0.5393 - acc: 0.7143 - val_loss: 0.5815 - val_acc: 0.7000\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.5344 - acc: 0.7296 - val_loss: 0.5596 - val_acc: 0.6800\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.5457 - acc: 0.7245 - val_loss: 0.5825 - val_acc: 0.6600\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 0.5568 - acc: 0.7041 - val_loss: 0.5547 - val_acc: 0.7000\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 2s 415ms/step - loss: 0.5296 - acc: 0.7143 - val_loss: 0.5418 - val_acc: 0.6800\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.5171 - acc: 0.7398 - val_loss: 0.5329 - val_acc: 0.7000\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.5126 - acc: 0.7449 - val_loss: 0.5157 - val_acc: 0.7400\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.5098 - acc: 0.7551 - val_loss: 0.5231 - val_acc: 0.7600\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.5176 - acc: 0.7500 - val_loss: 0.4897 - val_acc: 0.7400\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.4966 - acc: 0.7602 - val_loss: 0.5206 - val_acc: 0.7600\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.5140 - acc: 0.7296 - val_loss: 0.5181 - val_acc: 0.7400\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.4623 - acc: 0.7704 - val_loss: 0.4872 - val_acc: 0.7800\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 2s 424ms/step - loss: 0.4833 - acc: 0.7602 - val_loss: 0.5041 - val_acc: 0.7600\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.4661 - acc: 0.7755 - val_loss: 0.4924 - val_acc: 0.7400\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.4593 - acc: 0.7959 - val_loss: 0.4928 - val_acc: 0.7600\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.4475 - acc: 0.7755 - val_loss: 0.4807 - val_acc: 0.7600\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 2s 423ms/step - loss: 0.4580 - acc: 0.7653 - val_loss: 0.5301 - val_acc: 0.7400\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.4709 - acc: 0.7500 - val_loss: 0.5608 - val_acc: 0.7200\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.4777 - acc: 0.7806 - val_loss: 0.5114 - val_acc: 0.7600\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 3s 807ms/step - loss: 0.4744 - acc: 0.7806 - val_loss: 0.4983 - val_acc: 0.7600\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 2s 435ms/step - loss: 0.4518 - acc: 0.7806 - val_loss: 0.4858 - val_acc: 0.7800\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 2s 418ms/step - loss: 0.4481 - acc: 0.7806 - val_loss: 0.4802 - val_acc: 0.7800\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 2s 538ms/step - loss: 0.4489 - acc: 0.7857 - val_loss: 0.4840 - val_acc: 0.7400\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 2s 426ms/step - loss: 0.4432 - acc: 0.7755 - val_loss: 0.4948 - val_acc: 0.7600\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.4341 - acc: 0.7857 - val_loss: 0.4917 - val_acc: 0.7800\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.4484 - acc: 0.7755 - val_loss: 0.5101 - val_acc: 0.7600\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.4197 - acc: 0.7908 - val_loss: 0.4770 - val_acc: 0.7800\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.4178 - acc: 0.7959 - val_loss: 0.4822 - val_acc: 0.7600\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 2s 539ms/step - loss: 0.4068 - acc: 0.8112 - val_loss: 0.4838 - val_acc: 0.8000\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.3978 - acc: 0.7908 - val_loss: 0.4850 - val_acc: 0.8200\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.4048 - acc: 0.8214 - val_loss: 0.4862 - val_acc: 0.7800\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 2s 424ms/step - loss: 0.3875 - acc: 0.8112 - val_loss: 0.4885 - val_acc: 0.7800\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 2s 538ms/step - loss: 0.4067 - acc: 0.8265 - val_loss: 0.4987 - val_acc: 0.7400\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.3876 - acc: 0.8010 - val_loss: 0.5155 - val_acc: 0.7600\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.3824 - acc: 0.8214 - val_loss: 0.5611 - val_acc: 0.7400\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 2s 436ms/step - loss: 0.3865 - acc: 0.8061 - val_loss: 0.5087 - val_acc: 0.7800\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 2s 534ms/step - loss: 0.3535 - acc: 0.8469 - val_loss: 0.5229 - val_acc: 0.7600\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.3634 - acc: 0.8469 - val_loss: 0.5720 - val_acc: 0.7000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.3653 - acc: 0.8418 - val_loss: 0.5780 - val_acc: 0.7400\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.3656 - acc: 0.8265 - val_loss: 0.5353 - val_acc: 0.7400\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 2s 662ms/step - loss: 0.3703 - acc: 0.8316 - val_loss: 0.5159 - val_acc: 0.7600\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 3s 734ms/step - loss: 0.4198 - acc: 0.7908 - val_loss: 0.5184 - val_acc: 0.7800\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.4310 - acc: 0.8112 - val_loss: 0.6268 - val_acc: 0.7000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.3911 - acc: 0.8061 - val_loss: 0.6234 - val_acc: 0.6800\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 2s 443ms/step - loss: 0.3844 - acc: 0.8265 - val_loss: 0.5589 - val_acc: 0.7400\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.3752 - acc: 0.8061 - val_loss: 0.5070 - val_acc: 0.7600\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.3604 - acc: 0.8214 - val_loss: 0.5801 - val_acc: 0.7000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.3819 - acc: 0.8163 - val_loss: 0.5724 - val_acc: 0.7200\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 2s 437ms/step - loss: 0.3853 - acc: 0.8061 - val_loss: 0.5228 - val_acc: 0.7000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 2s 457ms/step - loss: 0.3590 - acc: 0.8367 - val_loss: 0.5719 - val_acc: 0.7000\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.3413 - acc: 0.8571 - val_loss: 0.5467 - val_acc: 0.7400\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.3490 - acc: 0.8469 - val_loss: 0.5308 - val_acc: 0.7200\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.3390 - acc: 0.8776 - val_loss: 0.5907 - val_acc: 0.7000\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.3461 - acc: 0.8367 - val_loss: 0.5567 - val_acc: 0.7000\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 2s 457ms/step - loss: 0.3534 - acc: 0.8571 - val_loss: 0.5513 - val_acc: 0.7600\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 2s 432ms/step - loss: 0.3367 - acc: 0.8622 - val_loss: 0.6328 - val_acc: 0.6600\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 2s 418ms/step - loss: 0.3396 - acc: 0.8418 - val_loss: 0.5930 - val_acc: 0.7200\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 2s 426ms/step - loss: 0.3243 - acc: 0.8520 - val_loss: 0.5766 - val_acc: 0.7000\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 2s 448ms/step - loss: 0.3213 - acc: 0.8571 - val_loss: 0.5351 - val_acc: 0.8000\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 3s 739ms/step - loss: 0.2880 - acc: 0.8776 - val_loss: 0.5652 - val_acc: 0.7000\n",
            "50/50 [==============================] - 0s 7ms/step - loss: 0.4850 - acc: 0.8200\n"
          ]
        }
      ],
      "source": [
        "model = Model(inputs=x_inp, outputs=predictions)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005), loss=binary_crossentropy, metrics=[\"acc\"],\n",
        ")\n",
        "\n",
        "train_graphs, test_graphs = model_selection.train_test_split(graph_labels, train_size=0.8, test_size=0.2, stratify=graph_labels, random_state=rs)\n",
        "gen = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "train_gen = gen.flow(\n",
        "    list(train_graphs.index - 1),\n",
        "    targets=train_graphs.values,\n",
        "    batch_size=50,\n",
        "    symmetric_normalization=True,\n",
        ")\n",
        "\n",
        "test_gen = gen.flow(\n",
        "    list(test_graphs.index - 1),\n",
        "    targets=test_graphs.values,\n",
        "    batch_size=1,\n",
        "    symmetric_normalization=True,\n",
        ")\n",
        "\n",
        "history = model.fit(train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True, callbacks=[es],)\n",
        "test_metrics = model.evaluate(test_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgNzNEo3pOP9",
        "outputId": "be101180-5b96-45b5-954f-f8d6272d69cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Metrics:\n",
            "Validation Accuracy:  0.699999988079071\n",
            "Validation Loss:  0.559349775314331\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTest Set Metrics:\")\n",
        "\n",
        "print(\"Validation Accuracy: \", max(history.history['val_acc']))\n",
        "print(\"Validation Loss: \", min(history.history['val_loss']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAjTlBZ8WOM3",
        "outputId": "2c6291d9-e2e0-4a93-fa61-02391cca459e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Set Metrics:\n",
            "Accuracy:  0.8418367505073547\n",
            "Loss:  0.4243176579475403\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTraining Set Metrics:\")\n",
        "\n",
        "print(\"Accuracy: \", max(history.history['acc']))\n",
        "print(\"Loss: \", min(history.history['loss']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "p6mG-b0yPVvI",
        "outputId": "82da2d06-881a-408c-86dd-6f6ce0a4bd68"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train acc', 'val acc'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHHZOS3e0XN3",
        "outputId": "cbd79d79-5bfb-4f8d-d406-dec3d02deda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 1s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(test_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a0R8L8G4j7P"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "x = np.array(pred)\n",
        "for i in x:\n",
        "  if i <0.5:\n",
        "    y_pred.append(0)\n",
        "  else:\n",
        "    y_pred.append(1)\n",
        "\n",
        "for t in test_gen.targets:\n",
        "    y_true.append(t[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIph_bQG5Xoo"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, y_pred)\n",
        "precision = metrics.precision_score(y_true, y_pred)\n",
        "recall =  metrics.recall_score(y_true, y_pred)\n",
        "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "specificity = tn / (tn+fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jLoV_Out_Wi",
        "outputId": "a7191bb4-2c63-4204-aa64-d9f2973be06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics:\n",
            "Accuracy:  0.82\n",
            "f1 score:  0.7999999999999999\n",
            "precision:  0.9\n",
            "recall:  0.72\n",
            "specificity  0.92\n"
          ]
        }
      ],
      "source": [
        "print(\"Metrics:\")\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"f1 score: \", f1_score)\n",
        "print(\"precision: \", precision)\n",
        "print(\"recall: \", recall)\n",
        "print(\"specificity \", specificity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GjpsfWayrA67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acc: 80.4 +- 4.5 <br>\n",
        "f1: 79.0 +- 4.4 <br>\n",
        "precision: <br>\n",
        "recall: <br>\n",
        "specificity: <br>\n"
      ],
      "metadata": {
        "id": "1MOHWv80rBoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [ 0.86, 0.72, 0.75, 0.863, 0.789, 0.78, 0.8, 0.79, 0.76 ]\n",
        "np.std (acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swfSSJbPoF7u",
        "outputId": "c6279039-0159-4bd7-d04a-6167d9091feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04450911960930099"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N97oNgVtWuaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QOHMmZQkTGWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "x25DMyGRiCmJ",
        "outputId": "848d3b7e-3f74-4e8a-ab86-40b3a5027abf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f7e7fdf9050>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAAEGCAYAAAD45CnNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZRklEQVR4nO3de5wV5Z3n8c+3G/CCeEEEEVCMEpR1FJGgSYzrFYHNhpjJJJrMjjG6SqKTTDZuRmdng2tmd7OvXFx3jGNQWTSToHGjBkdXJGZm0V0TQUYTLhqIl3BT5KYIqHT3b/841XpouvtUNedw6pz6vl+vevWpp+pU/Rpe/Hieep56HkUEZmZF0FLvAMzM9hUnPDMrDCc8MysMJzwzKwwnPDMrjH71DqDc4MEtMWJka73DsAz+sOLQeodgGexs38a7HW9rb65x4TkDY9Pm9lTnPvObd+ZHxJS9uV815SrhjRjZyoMPD6l3GJbB1RM+Ue8QLIOntt6/19fYtLmdp+cfnerc1uErc/UPOlcJz8zyL4AOOuodRp844ZlZJkGwK9I1afPGCc/MMnMNz8wKIQjaG/SVVCc8M8usAyc8MyuAANqd8MysKFzDM7NCCGCXn+GZWREE4SatmRVEQHtj5jsnPDPLpvSmRWNywjOzjEQ7ezX/QN044ZlZJqVOCyc8MyuA0jg8JzwzK4gO1/DMrAhcwzOzwghEe4OuDuGEZ2aZNWqTtjHTtJnVTSDejdZUW28kjZL0j5KWS1om6atJ+WBJCyStTH4e1sP3L03OWSnp0jSxO+GZWSalgcctqbYK2oCvR8Q44AzgaknjgOuAxyNiDPB4sr8bSYOBmcDpwCRgZk+JsZwTnpll1p4MPq609SYi1kfEkuTzNmAFMAKYDtyVnHYX8Mluvn4hsCAiNkfEFmABUHF1ND/DM7NMIkR7pK4rDZG0uGx/VkTM6nqSpNHAqcCvgWERsT459CowrJvrjgBWl+2vScp65YRnZpl1pB+WsjEiJvZ2gqSDgJ8BfxERb0rvXzsiQlLVpipwk9bMMil1WvRLtVUiqT+lZPfjiOhcNPc1ScOT48OBDd18dS0wqmx/ZFLWKyc8M8ukWp0WKlXl7gRWRMT3yw7NAzp7XS8Fft7N1+cDkyUdlnRWTE7KeuUmrZll1l6dcXgfBf4N8FtJzyZlfwV8G/ippMuBV4DPAEiaCMyIiCsiYrOkbwGLku/dGBGbK93QCc/MMqnWmxYR8ST0+DDwvG7OXwxcUbY/G5id5Z5OeGaWWUf6XtpcccIzs0xKkwc44ZlZAQRiV4XXxvLKCc/MMokgy8DjXHHCM7OMlGXgca444ZlZJoFreGZWIO60MLNCCNSwE4A64ZlZJqVlGhszdTRm1GZWR16I28wKIvCbFmZWIK7hmVkhRMg1PDMrhlKnhV8tM7NCyLSmRa444ZlZJqVOCz/DM7OC8JsWZlYIftPCzAql0gI9eeWEZ2aZRMCuDic8MyuAUpO2OglP0mzg48CGiDgpKbsXGJucciiwNSLGd/Pdl4FtQDvQVmnBb3DCM7M+qOKbFnOAW4C7Owsi4rOdnyV9D3ijl++fExEb097MCa+KNq8bwJyvfZA3Nw5ACs783Guc98V1PPPw4fzDTUfz6qoDuW7ecxxz8lv1DtV60dIS3HzPYjZt2I8brjm53uHkTjWHpUTEQkmjuzuWLNT9GeDcqtwMavvkUdIUSS9IWiXpulreKw9aW4NP//VL3PD4Ev7ywd/wf+4ezrrfHcBRH9zBVT98nuNPf7PeIVoK0/90NatfOrDeYeRYqUmbZgOGSFpctl2Z4UYfA16LiJU9HA/gMUnPpL1uzWp4klqBHwAXAGuARZLmRcTyWt2z3g4ZtotDhu0CYP+D2jny+B1sfW0/xn1sa50js7QOH/Y2H/rYJu69fTQX/dnqeoeTWxnWtNiY5tlaDy4B5vZy/MyIWCtpKLBA0vMRsbC3C9aySTsJWBURLwJIugeYDjRtwiu3cfV+rF42kGPHb6t3KJbBVd9YxeybjueAA9vqHUpulXppa/suraR+wKeA03qOI9YmPzdIeoBSzuk14dWySTsCKP8vck1SthtJV3ZWdzdv7qhhOPvO29tbmDXjRD7zzZc4YFB7vcOxlCadtZGtm/uzavmgeoeSa50Dj9Nse+F84PmIWNPdQUkDJQ3q/AxMBpZWumjdB9NExKyImBgREwcPrns4e619l5g140QmfXIDp07dVO9wLINxp77BGeds4n8++hR/+Z3lnDxpC9f+10I0SDLrSJZqrLRVImku8BQwVtIaSZcnhy6mS3NW0lGSHkl2hwFPSnoOeBp4OCIerXS/WjZp1wKjyvZHJmVNKwLu/sYYjjx+B+f/23X1DscymnPzccy5+TgA/mjiFv74C6v57vXj6hxV/lS5l/aSHsq/0E3ZOmBa8vlF4JSs96tlwlsEjJF0LKVEdzHwuRrer+5+v/hgfn3/UEacsJ2/mVoaJzn9379C27st3DvzA7y1uT+3XDaOUeO285UfLatztGZ95wlAu4iINknXAPOBVmB2RDT1v/LjP/Qmt73yZLfHTp3i5m0j+e3iw/jt4sPqHUYuRYg2J7w9RcQjwCMVTzSzhuLZUsysEDwBqJkVihOemRWCJwA1s0LJ8GpZrjjhmVkmEdDmCUDNrCjcpDWzQvAzPDMrlHDCM7OicKeFmRVChJ/hmVlhiHb30ppZUfgZnpkVgt+lNbPiiNJzvEbkhGdmmbmX1swKIRq406IxozazuopIt1UiabakDZKWlpXdIGmtpGeTbVoP350i6QVJqyRdlyZuJzwzyyxCqbYU5gBTuim/KSLGJ9ses6ZLagV+AEwFxgGXSKq44pITnpllUqq9VSfhRcRCYHMfwpgErIqIFyPiXeAeYHqlLznhmVlmGRbiHiJpcdl2ZcpbXCPpN0mTt7vVlEYAq8v21yRlvXLCM7PMMjzD2xgRE8u2WSku/3fAccB4YD3wvWrF7V5aM8skEB017KWNiNc6P0u6HfiHbk5bC4wq2x+ZlPXKNTwzyyxSbn0haXjZ7kXA0m5OWwSMkXSspAHAxcC8Std2Dc/MsonqvUsraS5wNqVnfWuAmcDZksaX7sTLwFXJuUcBd0TEtIhok3QNMB9oBWZHxLJK93PCM7PsqvRqWURc0k3xnT2cuw6YVrb/CLDHkJXeOOGZWWZNN1uKpL+llzweEV+pSURmlmsBdHQ0WcIDFu+zKMyscQTQbDW8iLirfF/SgRGxo/YhmVneNer0UBWHpUj6sKTlwPPJ/imSbq15ZGaWX7Ucl1JDacbh/XfgQmATQEQ8B5xVy6DMLM/SvUebx46NVL20EbFa2i349tqEY2YNIYe1tzTSJLzVkj4ChKT+wFeBFbUNy8xyKyAatJc2TZN2BnA1pZkI1lF6offqWgZlZnmnlFu+VKzhRcRG4PP7IBYzaxQN2qRN00v7AUkPSXo9mYr555I+sC+CM7OcauJe2p8APwWGA0cB9wFzaxmUmeVY58DjNFvOpEl4B0bEjyKiLdn+Hti/1oGZWX5VaxGffa23d2kHJx//d7Ii0D2UcvtnyThDgZk1mQbtpe2t0+IZSgmu8ze7quxYANfXKigzyzflsPaWRm/v0h67LwMxswaR0w6JNFK9aSHpJEprP7737C4i7q5VUGaWZ/nskEijYsKTNJPSFMzjKD27mwo8CTjhmRVVg9bw0vTSfho4D3g1Ii4DTgEOqWlUZpZvHSm3nEnTpN0ZER2S2iQdDGxg9+XRzKxImnEC0DKLJR0K3E6p5/Yt4KmaRmVmuVatXlpJs4GPAxsi4qSk7DvAvwbeBX4PXBYRW7v57svANkqzN7VFxMRK96vYpI2IL0fE1oi4DbgAuDRp2ppZUVXv1bI5wJQuZQuAkyLiZOB39D4E7pyIGJ8m2UHvA48n9HYsIpakuYGZWU8iYqGk0V3KHivb/RWlfoSq6K1J+71ejgVwbrWC6PTKbwcx45gzq31Zq6H5635Z7xAsg0kXbqvKdTI0aYdIKl8QbFZEzMpwqy8C9/ZwLIDHJAXwwzTX7W3g8TkZgjKzogiyvFq2MW1zsytJ/wFoA37cwylnRsRaSUOBBZKej4iFvV0zzbAUM7Pd1Xh6KElfoNSZ8fmI7qchiIi1yc8NwAPApErXdcIzs8wU6bY+XVuaAnwD+ERPS8NKGihpUOdnYDKwtNK1nfDMLLsq1fAkzaU0zG2spDWSLgduAQZRaqY+K+m25NyjJHXO1DQMeFLSc8DTwMMR8Wil+6V5tUyUpnj/QETcKOlo4MiIeLryr2NmTalK4/Ai4pJuiu/s4dx1wLTk84uU3vrKJE0N71bgw0BnYNuAH2S9kZk1h7TN2TxOIZXmTYvTI2KCpH8GiIgtkgbUOC4zy7MmnAC00y5JrSSVWElHkMvXgs1sX8lj7S2NNE3a/0Gpy3eopP9MaWqo/1LTqMws3xp01bI069L+WNIzlKaIEvDJiFhR88jMLJ9y+nwujTS9tEcDO4CHyssi4g+1DMzMcqxZEx7wMO8v5rM/cCzwAvAvahiXmeWYGvQpfpom7R+V7yezqHy5ZhGZmdVIqkV8ykXEEkmn1yIYM2sQzdqklfTvynZbgAnAuppFZGb51sydFpTeaevURumZ3s9qE46ZNYRmTHjJgONBEXHtPorHzBpBsyU8Sf0iok3SR/dlQGaWb6I5e2mfpvS87llJ84D7gO2dByPi/hrHZmZ51OTP8PYHNlFaw6JzPF4ATnhmRdWECW9o0kO7lPcTXacG/XXNrCoaNAP0lvBagYPYPdF1atBf18yqoRmbtOsj4sZ9FomZNY4mTHiNOcOfmdVWNGcv7Xn7LAozaywNWsPrcQLQiNi8LwMxs8ZRrTUtJM2WtEHS0rKywZIWSFqZ/Dysh+9empyzUtKlaeL2Mo1mll31ZjyeA0zpUnYd8HhEjAEeT/Z3I2kwMBM4ndIC3DN7SozlnPDMLJu0yS5FwouIhUDX1uR04K7k813AJ7v56oXAgojYHBFbgAXsmTj3kHl6KDMrNpFpWMoQSYvL9mdFxKwK3xkWEeuTz69SWnS7qxHA6rL9NUlZr5zwzCyzDAlvY0RM7Ot9IiKk6o36c5PWzLKr7aplr0kaDpD83NDNOWuBUWX7I5OyXjnhmVl2tU1484DOXtdLgZ93c858YLKkw5LOislJWa+c8Mwsm5RDUlIOS5kLPAWMlbRG0uXAt4ELJK0Ezk/2kTRR0h3w3rC5bwGLku3GNEPp/AzPzLKr0lO1iLikh0N7vPgQEYuBK8r2ZwOzs9zPCc/MMmvGV8vMzLrVjLOlmJntae86JOrKCc/MsnPCM7MiyPimRa444ZlZZupozIznhGdm2fgZnpkViZu0ZlYcTnhmVhSu4ZlZcTjhmVkhNOmqZWZme/A4PDMrlmjMjOeEZ2aZuYZne+i/Xwffu38V/QcErf2CJx4+lB9998h6h2VlNqztz3e+ejRbX+8PCqb96SYuumIjt994FL9acDD9BwTDj3mHr9+0moMOaa93uPnggcd7kjQb+DiwISJOqtV98mzXO+Ibf3Icb+9opbVf8P0HV7Hol4N4fsnAeodmidZ+wZXfXMeYk3ey460WrpnyQSactY0JZ23ji3+1jtZ+cMffDOeevx3KFX+9vvIFC6JROy1qOcX7HFKsE9ncxNs7WgHo1z9o7R+N+uijaR0+rI0xJ+8E4MCDOhh1/DtsXN+f087eRmtSHTjxtB1sXN+/jlHmjzrSbXlTsxpeRCyUNLpW128ULS3BLfN/x1Gj3+WhOYfzwj+7dpdXr64ewO+XHsAJE3bsVj5/7mD+5fStdYoqh4KG7bSo+yI+kq6UtFjS4l28U+9wqq6jQ3z5grF8/rRxjB2/g2PG7qx3SNaNndtb+NYVo5lx41oGDnq/avKTm4fR2i8491Nb6hhd/lRjER9JYyU9W7a9KekvupxztqQ3ys755t7EXfdOi2QV8lkAB2twY/63kcL2N1t57v8dxIfO2cYrLxxQ73CsTNsu+NYVozn3U1s4c9ob75U/du9gnv7FwXz73lVIdQwwj6rwLzUiXgDGA0hqpbSu7APdnPpERHx87++YgxpeMztkcBsDDy717A3Yv4MJZ73F6lX71zkqKxcB3//60Ywa8w5/fNXr75Uv+sdB3HfrUG6Y8yL7H9i0/w/3SefA42os01jmPOD3EfFKTYJO1L2G18wGD9vFtTf/gZYWaGmBhQ8dwq9/cXC9w7Iyy54eyOP/azDHnriTL50/FoDLrl/Hrf9xJLveEdd/9ngATjhtO1/9b2vqGWp+RGSZAHSIpMVl+7OSVl1XFwNze7jGhyU9B6wDro2IZemD3V0th6XMBc6m9AuvAWZGxJ21ul8evbTiAK6ePLbeYVgvTjp9O/PXPbtH+aTzVtQhmgaSvva2MSIm9naCpAHAJ4Druzm8BDgmIt6SNA14EBiTIdLd1LKXtqcFds2swVX5TYupwJKIeK3rgYh4s+zzI5JulTQkIjb25UZu0ppZNgFUd02LS+ihOSvpSOC1iAhJkyj1O2zq642c8MwsuyrlO0kDgQuAq8rKZgBExG3Ap4EvSWoDdgIXR/R9EKATnpllVq0mbURsBw7vUnZb2edbgFuqczcnPDPrAy/TaGbF4NlSzKwoSgOPGzPjOeGZWXY5nAklDSc8M8vMNTwzKwY/wzOz4sj0Lm2uOOGZWXZu0ppZIXghbjMrFNfwzKwwGjPfOeGZWXbqaMw2rROemWUTeOCxmRWDCA88NrMCccIzs8JwwjOzQvAzPDMrEvfSmllBhJu0ZlYQgROemRVIlVq0kl4GtgHtQFvXRbslCbgZmAbsAL4QEUv6ej8nPDPLrMrj8M7pZWHtqcCYZDsd+LvkZ5+09PWLZlZgEem2vTcduDtKfgUcKml4Xy/mhGdm2URAe0e6DYZIWly2Xdn1asBjkp7p5hjACGB12f6apKxP3KQ1s+zS1942dn0u18WZEbFW0lBggaTnI2Lh3gfYPdfwzCy7KjVpI2Jt8nMD8AAwqcspa4FRZfsjk7I+ccIzs2wC6Ih0Wy8kDZQ0qPMzMBlY2uW0ecCfqeQM4I2IWN/X0N2kNbOMAqIq41KGAQ+URp7QD/hJRDwqaQZARNwGPEJpSMoqSsNSLtubGzrhmVk2QWeHxN5dJuJF4JRuym8r+xzA1Xt9s4QTnpll5zctzKwwnPDMrBg8eYCZFUUAnh7KzArDNTwzK4aoSi9tPTjhmVk2AVGdcXj7nBOemWVX4S2KvHLCM7Ps/AzPzAohwr20ZlYgruGZWTEE0d5e7yD6xAnPzLLpnB6qATnhmVl2HpZiZkUQQLiGZ2aFEFWbAHSfc8Izs8watdNCkaPuZUmvA6/UO44aGAL0tNCw5VOz/p0dExFH7M0FJD1K6c8njY0RMWVv7ldNuUp4zUrS4gpL1VnO+O+sOXnVMjMrDCc8MysMJ7x9Y1a9A7DM/HfWhPwMz8wKwzU8MysMJzwzKwwnvBqSNEXSC5JWSbqu3vFYZZJmS9ogaWm9Y7Hqc8KrEUmtwA+AqcA44BJJ4+oblaUwB8jNQFmrLie82pkErIqIFyPiXeAeYHqdY7IKImIhsLnecVhtOOHVzghgddn+mqTMzOrECc/MCsMJr3bWAqPK9kcmZWZWJ054tbMIGCPpWEkDgIuBeXWOyazQnPBqJCLagGuA+cAK4KcRsay+UVklkuYCTwFjJa2RdHm9Y7Lq8atlZlYYruGZWWE44ZlZYTjhmVlhOOGZWWE44ZlZYTjhNRBJ7ZKelbRU0n2SDtyLa82R9Onk8x29TWwg6WxJH+nDPV6WtMfqVj2VdznnrYz3ukHStVljtGJxwmssOyNifEScBLwLzCg/KKlP6wxHxBURsbyXU84GMic8s7xxwmtcTwDHJ7WvJyTNA5ZLapX0HUmLJP1G0lUAKrklmZ/vF8DQzgtJ+idJE5PPUyQtkfScpMcljaaUWL+W1C4/JukIST9L7rFI0keT7x4u6TFJyyTdAajSLyHpQUnPJN+5ssuxm5LyxyUdkZQdJ+nR5DtPSDqhGn+YVgx9qhFYfSU1uanAo0nRBOCkiHgpSRpvRMSHJO0H/F9JjwGnAmMpzc03DFgOzO5y3SOA24GzkmsNjojNkm4D3oqI7ybn/QS4KSKelHQ0pbdJTgRmAk9GxI2S/hWQ5i2FLyb3OABYJOlnEbEJGAgsjoivSfpmcu1rKC2uMyMiVko6HbgVOLcPf4xWQE54jeUASc8mn58A7qTU1Hw6Il5KyicDJ3c+nwMOAcYAZwFzI6IdWCfpl91c/wxgYee1IqKneeHOB8ZJ71XgDpZ0UHKPTyXffVjSlhS/01ckXZR8HpXEugnoAO5Nyv8euD+5x0eA+8ruvV+Ke5gBTniNZmdEjC8vSP7hby8vAv48IuZ3OW9aFeNoAc6IiLe7iSU1SWdTSp4fjogdkv4J2L+H0yO579aufwZmafkZXvOZD3xJUn8ASR+UNBBYCHw2ecY3HDinm+/+CjhL0rHJdwcn5duAQWXnPQb8eeeOpM4EtBD4XFI2FTisQqyHAFuSZHcCpRpmpxags5b6OUpN5TeBlyT9SXIPSTqlwj3M3uOE13zuoPR8bkmyEM0PKdXkHwBWJsfupjQjyG4i4nXgSkrNx+d4v0n5EHBRZ6cF8BVgYtIpspz3e4v/E6WEuYxS0/YPFWJ9FOgnaQXwbUoJt9N2YFLyO5wL3JiUfx64PIlvGZ423zLwbClmVhiu4ZlZYTjhmVlhOOGZWWE44ZlZYTjhmVlhOOGZWWE44ZlZYfx/Rpb1h1b/d4gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred), display_labels=[0,1])\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJEuAkaBjh2H",
        "outputId": "fee75179-c89c-4f3c-a9d6-37ba0901d60d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.86"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roc_auc_score(y_true, y_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}